{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. INIT - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math as mt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pympler import asizeof\n",
    "from visdom import Visdom\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import networkx as nx\n",
    "#this is set for the printing of Q-matrices via console\n",
    "torch.set_printoptions(precision=3, sci_mode=False, linewidth=100)\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#suppress scientific notation in printouts\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. INIT - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_color_hex: str = \"#2f78b6\" # hex color of init point (left top)\n",
    "goal_color_hex: str = '#f0b226' # hex color of goal point (right bottom)\n",
    "wall_color: list[float] = [0.7, 0.7, 0.7] # RGB color of walls\n",
    "\n",
    "n_cell: int = 4 # how many original cells\n",
    "grid_dim = n_cell * 2 - 1 # side length of the square gridworld, in the form of 2n-1\n",
    "n_actions: int = 4 # how many actions are possible in each state\n",
    "lava: bool = False #do we use lava states - i.e. accessible wall states - (True) or wall states (False)? (we now always use \"False\")\n",
    "starting_state: int = 0\n",
    "goal_state = grid_dim ** 2 - 1\n",
    "\n",
    "# generation of maze\n",
    "maze_gen: bool = False # generate wall_states? wall_state_dict {0: [], 1: [1], 2: [2], 3: [3], ... } consists of \n",
    "n_mazes: int = 3800\n",
    "\n",
    "# rewards in the gridworld (-0.1, 100, -5)\n",
    "step_reward: float = -0.1 # for taking a step\n",
    "goal_reward: float = 2. # for reaching the goal\n",
    "wall_reward: float = -1. # for bumping into a wall\n",
    "\n",
    "# CA-TS Net settings\n",
    "input_neurons: int = 2 # for network init\n",
    "output_neurons = n_actions # modeling the Q(s, a) value by the output neurons w.r.t. number of action\n",
    "using_mazearray_as_concept: bool = False\n",
    "using_mazearray_as_concept_str = \"_fixedconcept\" if using_mazearray_as_concept else \"\"\n",
    "concept_size: int = 64 if using_mazearray_as_concept is False else grid_dim * grid_dim # the concept vector size of CA-TS DQN\n",
    "using_concept_eps: bool = True\n",
    "concept_eps: float = 1.0\n",
    "concept_eps_str = f\"_cpteps{concept_eps}\" if using_concept_eps else \"\"\n",
    "using_res: bool = True\n",
    "res_str = \"_res\" if using_res else \"\"\n",
    "hidden_dims: list = [768] * 18\n",
    "dim_str = \"-\".join(str(d) for d in hidden_dims)\n",
    "q_s2a: bool = False # whether using Q(s) -> a: True or Q(s, a): False\n",
    "q_str = \"\" if q_s2a else \"_sa2q\"\n",
    "n_agents: int = 5 # how many anegts used in communication games\n",
    "n_unique: int = 20 # unique mazes learned by each agent\n",
    "n_group: int = 8\n",
    "\n",
    "# CA-TS Net training settings\n",
    "batch_size: int = 512 # 0 indicates using all data in buffer as a batch\n",
    "epsilon: float = 0.1 # greedy action policy\n",
    "lr: float = 1e-4 # learning rate\n",
    "gamma_bellman: float = 0.9999 # bellman equation\n",
    "target_replace_steps: int = 0 # renew target_net by eval_net after how many iter times, 0 indicates directly using eval_net as target_net\n",
    "memory_capacity: int = 0 # number transitions stored, 0 indicates pre-store all transitions in memory (change training mode as epoch manner)\n",
    "cap_str = \"\" if memory_capacity != 0 else \"_prestore\"\n",
    "n_episode: int = 100000\n",
    "\n",
    "# AE Net settings\n",
    "n_recons_test: int = 20\n",
    "n_align_test: int = 20\n",
    "translater_dims: list = [1024] * 5\n",
    "translater_dim_str = \"-\".join(str(d) for d in translater_dims)\n",
    "\n",
    "# AE Net training settings\n",
    "translater_batch_size: int = 128\n",
    "translater_weight_decay: float = 1e-5\n",
    "translater_align_lr: float = 1e-5\n",
    "using_translater_eps: bool = False\n",
    "n_epoch: int = 10000\n",
    "print_frequency: int = 20\n",
    "train_translater: bool = True\n",
    "load_translater: bool = False\n",
    "loss_type: str = 'L1Loss' # options['MSELoss', 'L1Loss', 'SmoothL1Loss']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. FUNCTIONS - CA-TS Net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_sa2q(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_sa2q, self).__init__()\n",
    "        self.using_res = using_res\n",
    "\n",
    "        self.ts_fc_layers = nn.ModuleDict()\n",
    "        self.ts_norm_layers = nn.ModuleDict()\n",
    "        if self.using_res:\n",
    "            self.ts_skip_layers = nn.ModuleDict()\n",
    "\n",
    "        prev_dim = input_neurons + n_actions # dim 0, 1 is xy coordinates, dim 2 to 5 is action from 0 to 3 (right, uo, left, down)\n",
    "        for i, dim in enumerate(hidden_dims):\n",
    "            self.ts_fc_layers[f'fc{i}'] = nn.Linear(prev_dim, dim, bias=True)\n",
    "            if self.using_res and prev_dim != dim:\n",
    "                self.ts_skip_layers[f'skip{i}'] = nn.Linear(prev_dim, dim, bias=False)\n",
    "            self.ts_norm_layers[f'norm{i}'] = nn.LayerNorm(dim)\n",
    "            prev_dim = dim\n",
    "            \n",
    "\n",
    "        self.ts_fc_layers[f'fc{len(hidden_dims)}'] = nn.Linear(prev_dim, 1, bias=True)\n",
    "\n",
    "        self.cdp_fc_layers = nn.ModuleDict()\n",
    "        self.cdp_norm_layers = nn.ModuleDict()\n",
    "        if self.using_res:\n",
    "            self.cdp_skip_layers = nn.ModuleDict()\n",
    "\n",
    "        prev_dim = concept_size\n",
    "        for i, dim in enumerate(hidden_dims):\n",
    "            self.cdp_fc_layers[f'fc{i}'] = nn.Linear(prev_dim, dim, bias=True)\n",
    "            if self.using_res and prev_dim != dim:\n",
    "                self.cdp_skip_layers[f'skip{i}'] = nn.Linear(prev_dim, dim, bias=False)\n",
    "            self.cdp_norm_layers[f'norm{i}'] = nn.LayerNorm(dim)\n",
    "            prev_dim = dim\n",
    "\n",
    "        self.ts_afun = nn.ReLU()\n",
    "        self.cdp_afun = nn.Sigmoid()\n",
    "\n",
    "        self.concept_embedding_layer = nn.Embedding(num_embeddings=n_mazes, embedding_dim=concept_size)\n",
    "\n",
    "    def forward(self, x, concept_idx=None):\n",
    "        if concept_idx is not None:\n",
    "            concept = self.concept_embedding_layer(concept_idx)\n",
    "            cdp_activations = []\n",
    "            c = concept\n",
    "            for i in range(len(self.cdp_fc_layers)):\n",
    "                if self.using_res:\n",
    "                    identity = c\n",
    "                    out = self.cdp_fc_layers[f'fc{i}'](c)\n",
    "                    out = self.cdp_norm_layers[f'norm{i}'](out)\n",
    "                    if f'skip{i}' in self.cdp_skip_layers:\n",
    "                        identity = self.cdp_skip_layers[f'skip{i}'](identity)\n",
    "                    c = out + identity\n",
    "                else:\n",
    "                    c = self.cdp_fc_layers[f'fc{i}'](c)\n",
    "                    c = self.cdp_norm_layers[f'norm{i}'](c)\n",
    "\n",
    "                c = self.cdp_afun(c)\n",
    "                cdp_activations.append(c)\n",
    "\n",
    "        for i in range(len(self.ts_fc_layers) - 1):\n",
    "            if self.using_res:\n",
    "                identity = x\n",
    "                out = self.ts_fc_layers[f'fc{i}'](x)\n",
    "                out = self.ts_norm_layers[f'norm{i}'](out)\n",
    "                if f'skip{i}' in self.ts_skip_layers:\n",
    "                    identity = self.ts_skip_layers[f'skip{i}'](identity)\n",
    "                x = out + identity\n",
    "            else:\n",
    "                x = self.ts_fc_layers[f'fc{i}'](x)\n",
    "                x = self.ts_norm_layers[f'norm{i}'](x)\n",
    "                \n",
    "            x = self.ts_afun(x)\n",
    "\n",
    "            if concept_idx is not None:\n",
    "                x = torch.mul(x, cdp_activations[i])\n",
    "\n",
    "        x = self.ts_fc_layers[f'fc{len(self.cdp_fc_layers)}'](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. FUNCTIONS - Recons datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconsDataset(Dataset):\n",
    "    def __init__(self, concept_data):\n",
    "        self.concept_data = concept_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.concept_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.concept_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. FUNCTIONS - Alignment datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignDataset(Dataset):\n",
    "    def __init__(self, concept_data_list):\n",
    "        self.n_agents = len(concept_data_list)\n",
    "        self.concept_data_list = concept_data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.concept_data_list[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        grouped_item = []\n",
    "        for i in range(self.n_agents):\n",
    "            grouped_item.append(self.concept_data_list[i][idx])\n",
    "        return grouped_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. FUNCTIONS - Processing exclude mazes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recons_indices(n_mazes, n_agents, i_agent, n_unique, n_recons_test):\n",
    "    common_end = n_mazes - n_agents * n_unique\n",
    "    indices_list1 = list(range(0, common_end))\n",
    "    \n",
    "    unique_start = common_end + (i_agent - 1) * n_unique\n",
    "    unique_end = unique_start + n_unique\n",
    "    indices_list2 = list(range(unique_start, unique_end))\n",
    "\n",
    "    combined_list = indices_list1 + indices_list2\n",
    "    test_indices = random.sample(combined_list, n_recons_test)\n",
    "    train_indices = [item for item in combined_list if item not in test_indices]\n",
    "\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_align_indices(n_mazes, n_agents, n_unique, n_align_test):\n",
    "    common_end = n_mazes - n_agents * n_unique\n",
    "    indices_list = list(range(0, common_end))\n",
    "\n",
    "    test_indices = random.sample(indices_list, n_align_test)\n",
    "    train_indices = [item for item in indices_list if item not in test_indices]\n",
    "\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_indices(n_mazes, n_agents, i_agent, n_unique):\n",
    "    common_end = n_mazes - n_agents * n_unique\n",
    "    unique_start = common_end + (i_agent - 1) * n_unique\n",
    "    unique_end = unique_start + n_unique\n",
    "\n",
    "    return list(range(unique_start, unique_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_indices(n_mazes, n_agents, n_unique):\n",
    "    common_end = n_mazes - n_agents * n_unique\n",
    "\n",
    "    return list(range(0, common_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. FUNCTIONS - Translater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translater(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Translater, self).__init__()\n",
    "\n",
    "        self.trans_fc_layers = nn.ModuleDict()\n",
    "        self.trans_norm_layers = nn.ModuleDict()\n",
    "        self.trans_skip_layers = nn.ModuleDict()\n",
    "\n",
    "        prev_dim = concept_size\n",
    "        for i, dim in enumerate(translater_dims):\n",
    "            self.trans_fc_layers[f'fc{i}'] = nn.Linear(prev_dim, dim, bias=True)\n",
    "            if prev_dim != dim:\n",
    "                self.trans_skip_layers[f'skip{i}'] = nn.Linear(prev_dim, dim, bias=False)\n",
    "            self.trans_norm_layers[f'norm{i}'] = nn.LayerNorm(dim)\n",
    "            prev_dim = dim\n",
    "            \n",
    "        self.trans_fc_layers[f'fc{len(translater_dims)}'] = nn.Linear(prev_dim, concept_size, bias=True)\n",
    "        self.trans_afun = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.trans_fc_layers) - 1):\n",
    "            identity = x\n",
    "            out = self.trans_fc_layers[f'fc{i}'](x)\n",
    "            out = self.trans_norm_layers[f'norm{i}'](out)\n",
    "            if f'skip{i}' in self.trans_skip_layers:\n",
    "                identity = self.trans_skip_layers[f'skip{i}'](identity)\n",
    "            x = out + identity\n",
    "            x = self.trans_afun(x)\n",
    "\n",
    "        x = self.trans_fc_layers[f'fc{len(translater_dims)}'](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. EXECUTION - Constructing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2160173/1613587989.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agents[i_agent].load_state_dict(torch.load(ckpt_file))\n"
     ]
    }
   ],
   "source": [
    "agents = {}\n",
    "for i in range(n_agents):\n",
    "    i_agent = i + 1\n",
    "    agents[i_agent] = Net_sa2q()\n",
    "    ckpt_file: str = f\"agent_ckpt/group{n_group}/ckpt{i_agent}of{n_agents}_unique{n_unique}_{grid_dim}x{grid_dim}_n{n_mazes}{q_str}{cap_str}_dim{dim_str}{res_str}_cptsz{concept_size}{concept_eps_str}{using_mazearray_as_concept_str}_lr{lr}_epsi{n_episode}_gamma{gamma_bellman}_bs{batch_size}_tr{target_replace_steps}.pt\"\n",
    "    agents[i_agent].load_state_dict(torch.load(ckpt_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_trainsets = {}\n",
    "recons_trainloaders = {}\n",
    "recons_testsets = {}\n",
    "recons_testloaders = {}\n",
    "\n",
    "for i in range(n_agents):\n",
    "    i_agent = i + 1\n",
    "    recons_train_indices, recons_test_indices = get_recons_indices(n_mazes, n_agents, i_agent, n_unique, n_recons_test)\n",
    "    recons_trainsets[i_agent] = ReconsDataset(agents[i_agent].concept_embedding_layer.weight.data[recons_train_indices])\n",
    "    recons_testsets[i_agent] = ReconsDataset(agents[i_agent].concept_embedding_layer.weight.data[recons_test_indices])\n",
    "\n",
    "    recons_trainloaders[i_agent] = DataLoader(recons_trainsets[i_agent], batch_size = translater_batch_size, shuffle = True)\n",
    "    recons_testloaders[i_agent] = DataLoader(recons_testsets[i_agent], batch_size = translater_batch_size, shuffle = False)\n",
    "\n",
    "align_train_indices, align_test_indices = get_align_indices(n_mazes, n_agents, n_unique, n_align_test)\n",
    "align_train_concept = []\n",
    "align_test_concept = []\n",
    "for i in range(n_agents):\n",
    "    i_agent = i + 1\n",
    "    align_train_concept.append(agents[i_agent].concept_embedding_layer.weight.data[align_train_indices])\n",
    "    align_test_concept.append(agents[i_agent].concept_embedding_layer.weight.data[align_test_indices])\n",
    "\n",
    "align_trainset = AlignDataset(align_train_concept)\n",
    "align_testset = AlignDataset(align_test_concept)\n",
    "\n",
    "align_trainloader = DataLoader(align_trainset, batch_size = translater_batch_size, shuffle = True)\n",
    "align_testloader = DataLoader(align_testset, batch_size = translater_batch_size, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. EXECUTION - Translation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20,align training loss 0.9694,align testing loss 0.9920\n",
      "epoch 40,align training loss 0.9267,align testing loss 0.9965\n",
      "epoch 60,align training loss 0.8802,align testing loss 1.0079\n",
      "epoch 80,align training loss 0.8341,align testing loss 1.0217\n",
      "epoch 100,align training loss 0.7947,align testing loss 1.0332\n",
      "epoch 120,align training loss 0.7602,align testing loss 1.0373\n",
      "epoch 140,align training loss 0.7287,align testing loss 1.0483\n",
      "epoch 160,align training loss 0.7010,align testing loss 1.0573\n",
      "epoch 180,align training loss 0.6761,align testing loss 1.0666\n",
      "epoch 200,align training loss 0.6542,align testing loss 1.0734\n",
      "epoch 220,align training loss 0.6335,align testing loss 1.0808\n",
      "epoch 240,align training loss 0.6158,align testing loss 1.0879\n",
      "epoch 260,align training loss 0.5976,align testing loss 1.0951\n",
      "epoch 280,align training loss 0.5811,align testing loss 1.0976\n",
      "epoch 300,align training loss 0.5675,align testing loss 1.1042\n",
      "epoch 320,align training loss 0.5528,align testing loss 1.1059\n",
      "epoch 340,align training loss 0.5399,align testing loss 1.1073\n",
      "epoch 360,align training loss 0.5304,align testing loss 1.1165\n",
      "epoch 380,align training loss 0.5176,align testing loss 1.1238\n",
      "epoch 400,align training loss 0.5079,align testing loss 1.1237\n",
      "epoch 420,align training loss 0.4985,align testing loss 1.1245\n",
      "epoch 440,align training loss 0.4899,align testing loss 1.1269\n",
      "epoch 460,align training loss 0.4815,align testing loss 1.1248\n",
      "epoch 480,align training loss 0.4736,align testing loss 1.1279\n",
      "epoch 500,align training loss 0.4665,align testing loss 1.1296\n",
      "epoch 520,align training loss 0.4605,align testing loss 1.1320\n",
      "epoch 540,align training loss 0.4542,align testing loss 1.1305\n",
      "epoch 560,align training loss 0.4483,align testing loss 1.1375\n",
      "epoch 580,align training loss 0.4433,align testing loss 1.1372\n",
      "epoch 600,align training loss 0.4381,align testing loss 1.1359\n",
      "epoch 620,align training loss 0.4336,align testing loss 1.1337\n",
      "epoch 640,align training loss 0.4296,align testing loss 1.1342\n",
      "epoch 660,align training loss 0.4241,align testing loss 1.1359\n",
      "epoch 680,align training loss 0.4215,align testing loss 1.1348\n",
      "epoch 700,align training loss 0.4172,align testing loss 1.1358\n",
      "epoch 720,align training loss 0.4145,align testing loss 1.1363\n",
      "epoch 740,align training loss 0.4121,align testing loss 1.1344\n",
      "epoch 760,align training loss 0.4081,align testing loss 1.1344\n",
      "epoch 780,align training loss 0.4037,align testing loss 1.1359\n",
      "epoch 800,align training loss 0.4021,align testing loss 1.1336\n",
      "epoch 820,align training loss 0.3992,align testing loss 1.1303\n",
      "epoch 840,align training loss 0.3967,align testing loss 1.1307\n",
      "epoch 860,align training loss 0.3954,align testing loss 1.1359\n",
      "epoch 880,align training loss 0.3924,align testing loss 1.1393\n",
      "epoch 900,align training loss 0.3901,align testing loss 1.1283\n",
      "epoch 920,align training loss 0.3880,align testing loss 1.1341\n",
      "epoch 940,align training loss 0.3869,align testing loss 1.1287\n",
      "epoch 960,align training loss 0.3847,align testing loss 1.1302\n",
      "epoch 980,align training loss 0.3831,align testing loss 1.1355\n",
      "epoch 1000,align training loss 0.3827,align testing loss 1.1306\n",
      "epoch 1020,align training loss 0.3796,align testing loss 1.1256\n",
      "epoch 1040,align training loss 0.3783,align testing loss 1.1242\n",
      "epoch 1060,align training loss 0.3772,align testing loss 1.1249\n",
      "epoch 1080,align training loss 0.3761,align testing loss 1.1215\n",
      "epoch 1100,align training loss 0.3751,align testing loss 1.1206\n",
      "epoch 1120,align training loss 0.3729,align testing loss 1.1244\n",
      "epoch 1140,align training loss 0.3712,align testing loss 1.1194\n",
      "epoch 1160,align training loss 0.3708,align testing loss 1.1248\n",
      "epoch 1180,align training loss 0.3690,align testing loss 1.1228\n",
      "epoch 1200,align training loss 0.3689,align testing loss 1.1213\n",
      "epoch 1220,align training loss 0.3676,align testing loss 1.1170\n",
      "epoch 1240,align training loss 0.3656,align testing loss 1.1120\n",
      "epoch 1260,align training loss 0.3641,align testing loss 1.1152\n",
      "epoch 1280,align training loss 0.3642,align testing loss 1.1147\n",
      "epoch 1300,align training loss 0.3626,align testing loss 1.1183\n",
      "epoch 1320,align training loss 0.3614,align testing loss 1.1156\n",
      "epoch 1340,align training loss 0.3606,align testing loss 1.1147\n",
      "epoch 1360,align training loss 0.3590,align testing loss 1.1117\n",
      "epoch 1380,align training loss 0.3583,align testing loss 1.1115\n",
      "epoch 1400,align training loss 0.3590,align testing loss 1.1109\n",
      "epoch 1420,align training loss 0.3572,align testing loss 1.1166\n",
      "epoch 1440,align training loss 0.3563,align testing loss 1.1109\n",
      "epoch 1460,align training loss 0.3556,align testing loss 1.1110\n",
      "epoch 1480,align training loss 0.3558,align testing loss 1.1079\n",
      "epoch 1500,align training loss 0.3545,align testing loss 1.1082\n",
      "epoch 1520,align training loss 0.3544,align testing loss 1.1052\n",
      "epoch 1540,align training loss 0.3520,align testing loss 1.1051\n",
      "epoch 1560,align training loss 0.3510,align testing loss 1.1030\n",
      "epoch 1580,align training loss 0.3513,align testing loss 1.0955\n",
      "epoch 1600,align training loss 0.3519,align testing loss 1.1008\n",
      "epoch 1620,align training loss 0.3505,align testing loss 1.1001\n",
      "epoch 1640,align training loss 0.3506,align testing loss 1.0976\n",
      "epoch 1660,align training loss 0.3490,align testing loss 1.1026\n",
      "epoch 1680,align training loss 0.3481,align testing loss 1.1003\n",
      "epoch 1700,align training loss 0.3489,align testing loss 1.1021\n",
      "epoch 1720,align training loss 0.3474,align testing loss 1.1001\n",
      "epoch 1740,align training loss 0.3467,align testing loss 1.0978\n",
      "epoch 1760,align training loss 0.3469,align testing loss 1.0991\n",
      "epoch 1780,align training loss 0.3454,align testing loss 1.0967\n",
      "epoch 1800,align training loss 0.3460,align testing loss 1.0968\n",
      "epoch 1820,align training loss 0.3434,align testing loss 1.0980\n",
      "epoch 1840,align training loss 0.3447,align testing loss 1.0927\n",
      "epoch 1860,align training loss 0.3421,align testing loss 1.0917\n",
      "epoch 1880,align training loss 0.3432,align testing loss 1.0913\n",
      "epoch 1900,align training loss 0.3419,align testing loss 1.0974\n",
      "epoch 1920,align training loss 0.3422,align testing loss 1.0915\n",
      "epoch 1940,align training loss 0.3413,align testing loss 1.0918\n",
      "epoch 1960,align training loss 0.3413,align testing loss 1.0895\n",
      "epoch 1980,align training loss 0.3407,align testing loss 1.0873\n",
      "epoch 2000,align training loss 0.3393,align testing loss 1.0880\n",
      "epoch 2020,align training loss 0.3403,align testing loss 1.0882\n",
      "epoch 2040,align training loss 0.3406,align testing loss 1.0915\n",
      "epoch 2060,align training loss 0.3389,align testing loss 1.0864\n",
      "epoch 2080,align training loss 0.3380,align testing loss 1.0867\n",
      "epoch 2100,align training loss 0.3386,align testing loss 1.0849\n",
      "epoch 2120,align training loss 0.3371,align testing loss 1.0850\n",
      "epoch 2140,align training loss 0.3367,align testing loss 1.0844\n",
      "epoch 2160,align training loss 0.3370,align testing loss 1.0822\n",
      "epoch 2180,align training loss 0.3364,align testing loss 1.0761\n",
      "epoch 2200,align training loss 0.3362,align testing loss 1.0873\n",
      "epoch 2220,align training loss 0.3361,align testing loss 1.0873\n",
      "epoch 2240,align training loss 0.3351,align testing loss 1.0856\n",
      "epoch 2260,align training loss 0.3355,align testing loss 1.0828\n",
      "epoch 2280,align training loss 0.3332,align testing loss 1.0802\n",
      "epoch 2300,align training loss 0.3351,align testing loss 1.0827\n",
      "epoch 2320,align training loss 0.3343,align testing loss 1.0794\n",
      "epoch 2340,align training loss 0.3332,align testing loss 1.0781\n",
      "epoch 2360,align training loss 0.3325,align testing loss 1.0812\n",
      "epoch 2380,align training loss 0.3328,align testing loss 1.0713\n",
      "epoch 2400,align training loss 0.3322,align testing loss 1.0832\n",
      "epoch 2420,align training loss 0.3319,align testing loss 1.0783\n",
      "epoch 2440,align training loss 0.3317,align testing loss 1.0781\n",
      "epoch 2460,align training loss 0.3310,align testing loss 1.0810\n",
      "epoch 2480,align training loss 0.3315,align testing loss 1.0765\n",
      "epoch 2500,align training loss 0.3304,align testing loss 1.0771\n",
      "epoch 2520,align training loss 0.3308,align testing loss 1.0775\n",
      "epoch 2540,align training loss 0.3305,align testing loss 1.0788\n",
      "epoch 2560,align training loss 0.3303,align testing loss 1.0827\n",
      "epoch 2580,align training loss 0.3301,align testing loss 1.0752\n",
      "epoch 2600,align training loss 0.3293,align testing loss 1.0796\n",
      "epoch 2620,align training loss 0.3282,align testing loss 1.0742\n",
      "epoch 2640,align training loss 0.3287,align testing loss 1.0720\n",
      "epoch 2660,align training loss 0.3287,align testing loss 1.0728\n",
      "epoch 2680,align training loss 0.3284,align testing loss 1.0750\n",
      "epoch 2700,align training loss 0.3281,align testing loss 1.0728\n",
      "epoch 2720,align training loss 0.3282,align testing loss 1.0677\n",
      "epoch 2740,align training loss 0.3268,align testing loss 1.0709\n",
      "epoch 2760,align training loss 0.3263,align testing loss 1.0721\n",
      "epoch 2780,align training loss 0.3264,align testing loss 1.0689\n",
      "epoch 2800,align training loss 0.3268,align testing loss 1.0634\n",
      "epoch 2820,align training loss 0.3257,align testing loss 1.0680\n",
      "epoch 2840,align training loss 0.3271,align testing loss 1.0645\n",
      "epoch 2860,align training loss 0.3247,align testing loss 1.0714\n",
      "epoch 2880,align training loss 0.3248,align testing loss 1.0707\n",
      "epoch 2900,align training loss 0.3244,align testing loss 1.0689\n",
      "epoch 2920,align training loss 0.3249,align testing loss 1.0665\n",
      "epoch 2940,align training loss 0.3247,align testing loss 1.0679\n",
      "epoch 2960,align training loss 0.3241,align testing loss 1.0665\n",
      "epoch 2980,align training loss 0.3242,align testing loss 1.0667\n",
      "epoch 3000,align training loss 0.3240,align testing loss 1.0662\n",
      "epoch 3020,align training loss 0.3240,align testing loss 1.0678\n",
      "epoch 3040,align training loss 0.3236,align testing loss 1.0713\n",
      "epoch 3060,align training loss 0.3234,align testing loss 1.0672\n",
      "epoch 3080,align training loss 0.3229,align testing loss 1.0658\n",
      "epoch 3100,align training loss 0.3225,align testing loss 1.0710\n",
      "epoch 3120,align training loss 0.3233,align testing loss 1.0655\n",
      "epoch 3140,align training loss 0.3221,align testing loss 1.0615\n",
      "epoch 3160,align training loss 0.3213,align testing loss 1.0649\n",
      "epoch 3180,align training loss 0.3218,align testing loss 1.0647\n",
      "epoch 3200,align training loss 0.3217,align testing loss 1.0639\n",
      "epoch 3220,align training loss 0.3215,align testing loss 1.0686\n",
      "epoch 3240,align training loss 0.3207,align testing loss 1.0629\n",
      "epoch 3260,align training loss 0.3209,align testing loss 1.0611\n",
      "epoch 3280,align training loss 0.3211,align testing loss 1.0598\n",
      "epoch 3300,align training loss 0.3221,align testing loss 1.0615\n",
      "epoch 3320,align training loss 0.3211,align testing loss 1.0606\n",
      "epoch 3340,align training loss 0.3201,align testing loss 1.0566\n",
      "epoch 3360,align training loss 0.3206,align testing loss 1.0592\n",
      "epoch 3380,align training loss 0.3191,align testing loss 1.0633\n",
      "epoch 3400,align training loss 0.3203,align testing loss 1.0553\n",
      "epoch 3420,align training loss 0.3198,align testing loss 1.0604\n",
      "epoch 3440,align training loss 0.3184,align testing loss 1.0643\n",
      "epoch 3460,align training loss 0.3193,align testing loss 1.0586\n",
      "epoch 3480,align training loss 0.3192,align testing loss 1.0600\n",
      "epoch 3500,align training loss 0.3184,align testing loss 1.0632\n",
      "epoch 3520,align training loss 0.3180,align testing loss 1.0621\n",
      "epoch 3540,align training loss 0.3180,align testing loss 1.0641\n",
      "epoch 3560,align training loss 0.3170,align testing loss 1.0628\n",
      "epoch 3580,align training loss 0.3176,align testing loss 1.0577\n",
      "epoch 3600,align training loss 0.3182,align testing loss 1.0573\n",
      "epoch 3620,align training loss 0.3173,align testing loss 1.0580\n",
      "epoch 3640,align training loss 0.3171,align testing loss 1.0562\n",
      "epoch 3660,align training loss 0.3171,align testing loss 1.0606\n",
      "epoch 3680,align training loss 0.3162,align testing loss 1.0582\n",
      "epoch 3700,align training loss 0.3171,align testing loss 1.0609\n",
      "epoch 3720,align training loss 0.3169,align testing loss 1.0544\n",
      "epoch 3740,align training loss 0.3161,align testing loss 1.0546\n",
      "epoch 3760,align training loss 0.3160,align testing loss 1.0605\n",
      "epoch 3780,align training loss 0.3163,align testing loss 1.0579\n",
      "epoch 3800,align training loss 0.3153,align testing loss 1.0615\n",
      "epoch 3820,align training loss 0.3163,align testing loss 1.0580\n",
      "epoch 3840,align training loss 0.3153,align testing loss 1.0567\n",
      "epoch 3860,align training loss 0.3159,align testing loss 1.0562\n",
      "epoch 3880,align training loss 0.3148,align testing loss 1.0582\n",
      "epoch 3900,align training loss 0.3155,align testing loss 1.0566\n",
      "epoch 3920,align training loss 0.3143,align testing loss 1.0567\n",
      "epoch 3940,align training loss 0.3142,align testing loss 1.0584\n",
      "epoch 3960,align training loss 0.3145,align testing loss 1.0615\n",
      "epoch 3980,align training loss 0.3145,align testing loss 1.0569\n",
      "epoch 4000,align training loss 0.3139,align testing loss 1.0566\n",
      "epoch 4020,align training loss 0.3140,align testing loss 1.0554\n",
      "epoch 4040,align training loss 0.3134,align testing loss 1.0548\n",
      "epoch 4060,align training loss 0.3141,align testing loss 1.0579\n",
      "epoch 4080,align training loss 0.3135,align testing loss 1.0523\n",
      "epoch 4100,align training loss 0.3132,align testing loss 1.0524\n",
      "epoch 4120,align training loss 0.3142,align testing loss 1.0575\n",
      "epoch 4140,align training loss 0.3129,align testing loss 1.0572\n",
      "epoch 4160,align training loss 0.3123,align testing loss 1.0567\n",
      "epoch 4180,align training loss 0.3128,align testing loss 1.0524\n",
      "epoch 4200,align training loss 0.3141,align testing loss 1.0531\n",
      "epoch 4220,align training loss 0.3124,align testing loss 1.0561\n",
      "epoch 4240,align training loss 0.3128,align testing loss 1.0541\n",
      "epoch 4260,align training loss 0.3123,align testing loss 1.0507\n",
      "epoch 4280,align training loss 0.3116,align testing loss 1.0501\n",
      "epoch 4300,align training loss 0.3121,align testing loss 1.0515\n",
      "epoch 4320,align training loss 0.3111,align testing loss 1.0513\n",
      "epoch 4340,align training loss 0.3112,align testing loss 1.0539\n",
      "epoch 4360,align training loss 0.3117,align testing loss 1.0506\n",
      "epoch 4380,align training loss 0.3120,align testing loss 1.0475\n",
      "epoch 4400,align training loss 0.3113,align testing loss 1.0510\n",
      "epoch 4420,align training loss 0.3104,align testing loss 1.0539\n",
      "epoch 4440,align training loss 0.3116,align testing loss 1.0489\n",
      "epoch 4460,align training loss 0.3103,align testing loss 1.0450\n",
      "epoch 4480,align training loss 0.3095,align testing loss 1.0499\n",
      "epoch 4500,align training loss 0.3113,align testing loss 1.0466\n",
      "epoch 4520,align training loss 0.3108,align testing loss 1.0498\n",
      "epoch 4540,align training loss 0.3095,align testing loss 1.0499\n",
      "epoch 4560,align training loss 0.3091,align testing loss 1.0487\n",
      "epoch 4580,align training loss 0.3102,align testing loss 1.0520\n",
      "epoch 4600,align training loss 0.3092,align testing loss 1.0474\n",
      "epoch 4620,align training loss 0.3092,align testing loss 1.0518\n",
      "epoch 4640,align training loss 0.3102,align testing loss 1.0524\n",
      "epoch 4660,align training loss 0.3089,align testing loss 1.0472\n",
      "epoch 4680,align training loss 0.3095,align testing loss 1.0483\n",
      "epoch 4700,align training loss 0.3093,align testing loss 1.0499\n",
      "epoch 4720,align training loss 0.3088,align testing loss 1.0512\n",
      "epoch 4740,align training loss 0.3086,align testing loss 1.0503\n",
      "epoch 4760,align training loss 0.3091,align testing loss 1.0463\n",
      "epoch 4780,align training loss 0.3085,align testing loss 1.0499\n",
      "epoch 4800,align training loss 0.3089,align testing loss 1.0493\n",
      "epoch 4820,align training loss 0.3076,align testing loss 1.0468\n",
      "epoch 4840,align training loss 0.3085,align testing loss 1.0502\n",
      "epoch 4860,align training loss 0.3090,align testing loss 1.0487\n",
      "epoch 4880,align training loss 0.3079,align testing loss 1.0483\n",
      "epoch 4900,align training loss 0.3085,align testing loss 1.0483\n",
      "epoch 4920,align training loss 0.3086,align testing loss 1.0530\n",
      "epoch 4940,align training loss 0.3078,align testing loss 1.0472\n",
      "epoch 4960,align training loss 0.3084,align testing loss 1.0481\n",
      "epoch 4980,align training loss 0.3063,align testing loss 1.0452\n",
      "epoch 5000,align training loss 0.3077,align testing loss 1.0496\n",
      "epoch 5020,align training loss 0.3063,align testing loss 1.0474\n",
      "epoch 5040,align training loss 0.3076,align testing loss 1.0452\n",
      "epoch 5060,align training loss 0.3075,align testing loss 1.0448\n",
      "epoch 5080,align training loss 0.3070,align testing loss 1.0476\n",
      "epoch 5100,align training loss 0.3072,align testing loss 1.0463\n",
      "epoch 5120,align training loss 0.3063,align testing loss 1.0437\n",
      "epoch 5140,align training loss 0.3073,align testing loss 1.0460\n",
      "epoch 5160,align training loss 0.3072,align testing loss 1.0472\n",
      "epoch 5180,align training loss 0.3062,align testing loss 1.0433\n",
      "epoch 5200,align training loss 0.3060,align testing loss 1.0437\n",
      "epoch 5220,align training loss 0.3064,align testing loss 1.0413\n",
      "epoch 5240,align training loss 0.3066,align testing loss 1.0434\n",
      "epoch 5260,align training loss 0.3066,align testing loss 1.0436\n",
      "epoch 5280,align training loss 0.3064,align testing loss 1.0448\n",
      "epoch 5300,align training loss 0.3064,align testing loss 1.0463\n",
      "epoch 5320,align training loss 0.3062,align testing loss 1.0423\n",
      "epoch 5340,align training loss 0.3053,align testing loss 1.0397\n",
      "epoch 5360,align training loss 0.3058,align testing loss 1.0421\n",
      "epoch 5380,align training loss 0.3053,align testing loss 1.0412\n",
      "epoch 5400,align training loss 0.3047,align testing loss 1.0414\n",
      "epoch 5420,align training loss 0.3056,align testing loss 1.0438\n",
      "epoch 5440,align training loss 0.3050,align testing loss 1.0412\n",
      "epoch 5460,align training loss 0.3053,align testing loss 1.0411\n",
      "epoch 5480,align training loss 0.3050,align testing loss 1.0449\n",
      "epoch 5500,align training loss 0.3042,align testing loss 1.0425\n",
      "epoch 5520,align training loss 0.3049,align testing loss 1.0422\n",
      "epoch 5540,align training loss 0.3047,align testing loss 1.0387\n",
      "epoch 5560,align training loss 0.3042,align testing loss 1.0384\n",
      "epoch 5580,align training loss 0.3053,align testing loss 1.0406\n",
      "epoch 5600,align training loss 0.3051,align testing loss 1.0437\n",
      "epoch 5620,align training loss 0.3049,align testing loss 1.0427\n",
      "epoch 5640,align training loss 0.3041,align testing loss 1.0418\n",
      "epoch 5660,align training loss 0.3043,align testing loss 1.0464\n",
      "epoch 5680,align training loss 0.3046,align testing loss 1.0421\n",
      "epoch 5700,align training loss 0.3044,align testing loss 1.0427\n",
      "epoch 5720,align training loss 0.3043,align testing loss 1.0409\n",
      "epoch 5740,align training loss 0.3033,align testing loss 1.0403\n",
      "epoch 5760,align training loss 0.3026,align testing loss 1.0406\n",
      "epoch 5780,align training loss 0.3031,align testing loss 1.0446\n",
      "epoch 5800,align training loss 0.3037,align testing loss 1.0407\n",
      "epoch 5820,align training loss 0.3040,align testing loss 1.0383\n",
      "epoch 5840,align training loss 0.3028,align testing loss 1.0396\n",
      "epoch 5860,align training loss 0.3037,align testing loss 1.0388\n",
      "epoch 5880,align training loss 0.3034,align testing loss 1.0427\n",
      "epoch 5900,align training loss 0.3032,align testing loss 1.0424\n",
      "epoch 5920,align training loss 0.3021,align testing loss 1.0425\n",
      "epoch 5940,align training loss 0.3037,align testing loss 1.0434\n",
      "epoch 5960,align training loss 0.3025,align testing loss 1.0401\n",
      "epoch 5980,align training loss 0.3029,align testing loss 1.0347\n",
      "epoch 6000,align training loss 0.3037,align testing loss 1.0390\n",
      "epoch 6020,align training loss 0.3030,align testing loss 1.0408\n",
      "epoch 6040,align training loss 0.3034,align testing loss 1.0394\n",
      "epoch 6060,align training loss 0.3024,align testing loss 1.0359\n",
      "epoch 6080,align training loss 0.3020,align testing loss 1.0339\n",
      "epoch 6100,align training loss 0.3019,align testing loss 1.0361\n",
      "epoch 6120,align training loss 0.3018,align testing loss 1.0341\n",
      "epoch 6140,align training loss 0.3028,align testing loss 1.0392\n",
      "epoch 6160,align training loss 0.3030,align testing loss 1.0366\n",
      "epoch 6180,align training loss 0.3015,align testing loss 1.0375\n",
      "epoch 6200,align training loss 0.3020,align testing loss 1.0373\n",
      "epoch 6220,align training loss 0.3012,align testing loss 1.0375\n",
      "epoch 6240,align training loss 0.3022,align testing loss 1.0340\n",
      "epoch 6260,align training loss 0.3017,align testing loss 1.0393\n",
      "epoch 6280,align training loss 0.3013,align testing loss 1.0395\n",
      "epoch 6300,align training loss 0.3023,align testing loss 1.0342\n",
      "epoch 6320,align training loss 0.3016,align testing loss 1.0351\n",
      "epoch 6340,align training loss 0.3006,align testing loss 1.0400\n",
      "epoch 6360,align training loss 0.3007,align testing loss 1.0382\n",
      "epoch 6380,align training loss 0.3016,align testing loss 1.0368\n",
      "epoch 6400,align training loss 0.3012,align testing loss 1.0392\n",
      "epoch 6420,align training loss 0.3008,align testing loss 1.0350\n",
      "epoch 6440,align training loss 0.3018,align testing loss 1.0312\n",
      "epoch 6460,align training loss 0.3012,align testing loss 1.0390\n",
      "epoch 6480,align training loss 0.3002,align testing loss 1.0337\n",
      "epoch 6500,align training loss 0.3005,align testing loss 1.0300\n",
      "epoch 6520,align training loss 0.3000,align testing loss 1.0393\n",
      "epoch 6540,align training loss 0.3009,align testing loss 1.0345\n",
      "epoch 6560,align training loss 0.3000,align testing loss 1.0354\n",
      "epoch 6580,align training loss 0.3000,align testing loss 1.0365\n",
      "epoch 6600,align training loss 0.3002,align testing loss 1.0367\n",
      "epoch 6620,align training loss 0.3001,align testing loss 1.0384\n",
      "epoch 6640,align training loss 0.2999,align testing loss 1.0376\n",
      "epoch 6660,align training loss 0.3003,align testing loss 1.0336\n",
      "epoch 6680,align training loss 0.3005,align testing loss 1.0366\n",
      "epoch 6700,align training loss 0.3004,align testing loss 1.0366\n",
      "epoch 6720,align training loss 0.2999,align testing loss 1.0377\n",
      "epoch 6740,align training loss 0.2989,align testing loss 1.0373\n",
      "epoch 6760,align training loss 0.2996,align testing loss 1.0391\n",
      "epoch 6780,align training loss 0.3003,align testing loss 1.0396\n",
      "epoch 6800,align training loss 0.2999,align testing loss 1.0324\n",
      "epoch 6820,align training loss 0.2992,align testing loss 1.0358\n",
      "epoch 6840,align training loss 0.2997,align testing loss 1.0371\n",
      "epoch 6860,align training loss 0.2999,align testing loss 1.0339\n",
      "epoch 6880,align training loss 0.3001,align testing loss 1.0350\n",
      "epoch 6900,align training loss 0.2999,align testing loss 1.0365\n",
      "epoch 6920,align training loss 0.2998,align testing loss 1.0346\n",
      "epoch 6940,align training loss 0.2996,align testing loss 1.0318\n",
      "epoch 6960,align training loss 0.2985,align testing loss 1.0355\n",
      "epoch 6980,align training loss 0.2987,align testing loss 1.0362\n",
      "epoch 7000,align training loss 0.2994,align testing loss 1.0381\n",
      "epoch 7020,align training loss 0.2993,align testing loss 1.0348\n",
      "epoch 7040,align training loss 0.2985,align testing loss 1.0395\n",
      "epoch 7060,align training loss 0.2989,align testing loss 1.0368\n",
      "epoch 7080,align training loss 0.2991,align testing loss 1.0376\n",
      "epoch 7100,align training loss 0.2979,align testing loss 1.0335\n",
      "epoch 7120,align training loss 0.2987,align testing loss 1.0362\n",
      "epoch 7140,align training loss 0.2985,align testing loss 1.0343\n",
      "epoch 7160,align training loss 0.2980,align testing loss 1.0381\n",
      "epoch 7180,align training loss 0.2982,align testing loss 1.0347\n",
      "epoch 7200,align training loss 0.2977,align testing loss 1.0400\n",
      "epoch 7220,align training loss 0.2988,align testing loss 1.0409\n",
      "epoch 7240,align training loss 0.2989,align testing loss 1.0344\n",
      "epoch 7260,align training loss 0.2984,align testing loss 1.0304\n",
      "epoch 7280,align training loss 0.2979,align testing loss 1.0339\n",
      "epoch 7300,align training loss 0.2975,align testing loss 1.0314\n",
      "epoch 7320,align training loss 0.2980,align testing loss 1.0362\n",
      "epoch 7340,align training loss 0.2977,align testing loss 1.0333\n",
      "epoch 7360,align training loss 0.2976,align testing loss 1.0341\n",
      "epoch 7380,align training loss 0.2978,align testing loss 1.0348\n",
      "epoch 7400,align training loss 0.2979,align testing loss 1.0371\n",
      "epoch 7420,align training loss 0.2977,align testing loss 1.0327\n",
      "epoch 7440,align training loss 0.2973,align testing loss 1.0372\n",
      "epoch 7460,align training loss 0.2980,align testing loss 1.0350\n",
      "epoch 7480,align training loss 0.2974,align testing loss 1.0346\n",
      "epoch 7500,align training loss 0.2972,align testing loss 1.0361\n",
      "epoch 7520,align training loss 0.2980,align testing loss 1.0366\n",
      "epoch 7540,align training loss 0.2969,align testing loss 1.0349\n",
      "epoch 7560,align training loss 0.2971,align testing loss 1.0311\n",
      "epoch 7580,align training loss 0.2970,align testing loss 1.0320\n",
      "epoch 7600,align training loss 0.2986,align testing loss 1.0264\n",
      "epoch 7620,align training loss 0.2967,align testing loss 1.0333\n",
      "epoch 7640,align training loss 0.2968,align testing loss 1.0302\n",
      "epoch 7660,align training loss 0.2966,align testing loss 1.0292\n",
      "epoch 7680,align training loss 0.2974,align testing loss 1.0280\n",
      "epoch 7700,align training loss 0.2964,align testing loss 1.0328\n",
      "epoch 7720,align training loss 0.2968,align testing loss 1.0271\n",
      "epoch 7740,align training loss 0.2968,align testing loss 1.0382\n",
      "epoch 7760,align training loss 0.2968,align testing loss 1.0341\n",
      "epoch 7780,align training loss 0.2966,align testing loss 1.0331\n",
      "epoch 7800,align training loss 0.2962,align testing loss 1.0308\n",
      "epoch 7820,align training loss 0.2959,align testing loss 1.0297\n",
      "epoch 7840,align training loss 0.2967,align testing loss 1.0294\n",
      "epoch 7860,align training loss 0.2959,align testing loss 1.0299\n",
      "epoch 7880,align training loss 0.2960,align testing loss 1.0305\n",
      "epoch 7900,align training loss 0.2962,align testing loss 1.0287\n",
      "epoch 7920,align training loss 0.2955,align testing loss 1.0342\n",
      "epoch 7940,align training loss 0.2963,align testing loss 1.0297\n",
      "epoch 7960,align training loss 0.2962,align testing loss 1.0355\n",
      "epoch 7980,align training loss 0.2960,align testing loss 1.0287\n",
      "epoch 8000,align training loss 0.2952,align testing loss 1.0292\n",
      "epoch 8020,align training loss 0.2966,align testing loss 1.0310\n",
      "epoch 8040,align training loss 0.2953,align testing loss 1.0322\n",
      "epoch 8060,align training loss 0.2964,align testing loss 1.0321\n",
      "epoch 8080,align training loss 0.2957,align testing loss 1.0329\n",
      "epoch 8100,align training loss 0.2965,align testing loss 1.0311\n",
      "epoch 8120,align training loss 0.2954,align testing loss 1.0312\n",
      "epoch 8140,align training loss 0.2967,align testing loss 1.0305\n",
      "epoch 8160,align training loss 0.2967,align testing loss 1.0308\n",
      "epoch 8180,align training loss 0.2951,align testing loss 1.0314\n",
      "epoch 8200,align training loss 0.2950,align testing loss 1.0323\n",
      "epoch 8220,align training loss 0.2953,align testing loss 1.0323\n",
      "epoch 8240,align training loss 0.2957,align testing loss 1.0276\n",
      "epoch 8260,align training loss 0.2951,align testing loss 1.0324\n",
      "epoch 8280,align training loss 0.2958,align testing loss 1.0344\n",
      "epoch 8300,align training loss 0.2954,align testing loss 1.0309\n",
      "epoch 8320,align training loss 0.2951,align testing loss 1.0300\n",
      "epoch 8340,align training loss 0.2949,align testing loss 1.0308\n",
      "epoch 8360,align training loss 0.2948,align testing loss 1.0256\n",
      "epoch 8380,align training loss 0.2951,align testing loss 1.0340\n",
      "epoch 8400,align training loss 0.2947,align testing loss 1.0338\n",
      "epoch 8420,align training loss 0.2952,align testing loss 1.0295\n",
      "epoch 8440,align training loss 0.2950,align testing loss 1.0260\n",
      "epoch 8460,align training loss 0.2948,align testing loss 1.0314\n",
      "epoch 8480,align training loss 0.2949,align testing loss 1.0298\n",
      "epoch 8500,align training loss 0.2942,align testing loss 1.0275\n",
      "epoch 8520,align training loss 0.2937,align testing loss 1.0287\n",
      "epoch 8540,align training loss 0.2941,align testing loss 1.0263\n",
      "epoch 8560,align training loss 0.2944,align testing loss 1.0264\n",
      "epoch 8580,align training loss 0.2941,align testing loss 1.0277\n",
      "epoch 8600,align training loss 0.2938,align testing loss 1.0300\n",
      "epoch 8620,align training loss 0.2941,align testing loss 1.0303\n",
      "epoch 8640,align training loss 0.2947,align testing loss 1.0243\n",
      "epoch 8660,align training loss 0.2944,align testing loss 1.0275\n",
      "epoch 8680,align training loss 0.2942,align testing loss 1.0286\n",
      "epoch 8700,align training loss 0.2945,align testing loss 1.0335\n",
      "epoch 8720,align training loss 0.2940,align testing loss 1.0249\n",
      "epoch 8740,align training loss 0.2934,align testing loss 1.0224\n",
      "epoch 8760,align training loss 0.2944,align testing loss 1.0259\n",
      "epoch 8780,align training loss 0.2937,align testing loss 1.0271\n",
      "epoch 8800,align training loss 0.2939,align testing loss 1.0275\n",
      "epoch 8820,align training loss 0.2939,align testing loss 1.0298\n",
      "epoch 8840,align training loss 0.2951,align testing loss 1.0270\n",
      "epoch 8860,align training loss 0.2945,align testing loss 1.0269\n",
      "epoch 8880,align training loss 0.2945,align testing loss 1.0311\n",
      "epoch 8900,align training loss 0.2940,align testing loss 1.0268\n",
      "epoch 8920,align training loss 0.2933,align testing loss 1.0230\n",
      "epoch 8940,align training loss 0.2940,align testing loss 1.0283\n",
      "epoch 8960,align training loss 0.2942,align testing loss 1.0241\n",
      "epoch 8980,align training loss 0.2938,align testing loss 1.0269\n",
      "epoch 9000,align training loss 0.2933,align testing loss 1.0282\n",
      "epoch 9020,align training loss 0.2942,align testing loss 1.0287\n",
      "epoch 9040,align training loss 0.2937,align testing loss 1.0250\n",
      "epoch 9060,align training loss 0.2939,align testing loss 1.0277\n",
      "epoch 9080,align training loss 0.2924,align testing loss 1.0290\n",
      "epoch 9100,align training loss 0.2924,align testing loss 1.0251\n",
      "epoch 9120,align training loss 0.2932,align testing loss 1.0226\n",
      "epoch 9140,align training loss 0.2929,align testing loss 1.0261\n",
      "epoch 9160,align training loss 0.2934,align testing loss 1.0224\n",
      "epoch 9180,align training loss 0.2936,align testing loss 1.0220\n",
      "epoch 9200,align training loss 0.2924,align testing loss 1.0276\n",
      "epoch 9220,align training loss 0.2925,align testing loss 1.0240\n",
      "epoch 9240,align training loss 0.2928,align testing loss 1.0241\n",
      "epoch 9260,align training loss 0.2931,align testing loss 1.0273\n",
      "epoch 9280,align training loss 0.2921,align testing loss 1.0256\n",
      "epoch 9300,align training loss 0.2924,align testing loss 1.0251\n",
      "epoch 9320,align training loss 0.2927,align testing loss 1.0226\n",
      "epoch 9340,align training loss 0.2912,align testing loss 1.0217\n",
      "epoch 9360,align training loss 0.2929,align testing loss 1.0253\n",
      "epoch 9380,align training loss 0.2924,align testing loss 1.0196\n",
      "epoch 9400,align training loss 0.2919,align testing loss 1.0214\n",
      "epoch 9420,align training loss 0.2932,align testing loss 1.0239\n",
      "epoch 9440,align training loss 0.2922,align testing loss 1.0180\n",
      "epoch 9460,align training loss 0.2928,align testing loss 1.0234\n",
      "epoch 9480,align training loss 0.2920,align testing loss 1.0284\n",
      "epoch 9500,align training loss 0.2928,align testing loss 1.0250\n",
      "epoch 9520,align training loss 0.2924,align testing loss 1.0226\n",
      "epoch 9540,align training loss 0.2923,align testing loss 1.0264\n",
      "epoch 9560,align training loss 0.2919,align testing loss 1.0235\n",
      "epoch 9580,align training loss 0.2925,align testing loss 1.0232\n",
      "epoch 9600,align training loss 0.2929,align testing loss 1.0222\n",
      "epoch 9620,align training loss 0.2920,align testing loss 1.0255\n",
      "epoch 9640,align training loss 0.2917,align testing loss 1.0221\n",
      "epoch 9660,align training loss 0.2912,align testing loss 1.0243\n",
      "epoch 9680,align training loss 0.2919,align testing loss 1.0220\n",
      "epoch 9700,align training loss 0.2912,align testing loss 1.0232\n",
      "epoch 9720,align training loss 0.2924,align testing loss 1.0252\n",
      "epoch 9740,align training loss 0.2919,align testing loss 1.0237\n",
      "epoch 9760,align training loss 0.2917,align testing loss 1.0226\n",
      "epoch 9780,align training loss 0.2910,align testing loss 1.0231\n",
      "epoch 9800,align training loss 0.2925,align testing loss 1.0221\n",
      "epoch 9820,align training loss 0.2916,align testing loss 1.0250\n",
      "epoch 9840,align training loss 0.2914,align testing loss 1.0224\n",
      "epoch 9860,align training loss 0.2907,align testing loss 1.0217\n",
      "epoch 9880,align training loss 0.2913,align testing loss 1.0244\n",
      "epoch 9900,align training loss 0.2909,align testing loss 1.0274\n",
      "epoch 9920,align training loss 0.2914,align testing loss 1.0222\n",
      "epoch 9940,align training loss 0.2918,align testing loss 1.0262\n",
      "epoch 9960,align training loss 0.2915,align testing loss 1.0280\n",
      "epoch 9980,align training loss 0.2911,align testing loss 1.0238\n",
      "epoch 10000,align training loss 0.2916,align testing loss 1.0221\n"
     ]
    }
   ],
   "source": [
    "translater_2to1 = Translater().to(device)\n",
    "\n",
    "global_optim = torch.optim.Adam(translater_2to1.parameters(), lr = translater_align_lr, weight_decay = translater_weight_decay)\n",
    "recons_criterion = nn.__dict__[loss_type]()\n",
    "\n",
    "if train_translater:\n",
    "    try:\n",
    "        for epoch in range(n_epoch):\n",
    "            train_l_sum = 0.\n",
    "            train_batch_count = 0\n",
    "            test_l_sum = 0.\n",
    "            test_batch_count = 0\n",
    "            for x in align_trainloader:\n",
    "                concept_1 = x[0].to(device)\n",
    "                concept_1 += concept_eps * (torch.rand_like(concept_1).to(device) - 0.5)\n",
    "\n",
    "                concept_2 = x[1].to(device)\n",
    "                concept_2 += concept_eps * (torch.rand_like(concept_2).to(device) - 0.5)\n",
    "\n",
    "                concept_1_recons = translater_2to1(concept_2)\n",
    "                \n",
    "                l = recons_criterion(concept_1_recons, concept_1)\n",
    "                global_optim.zero_grad()\n",
    "                l.backward()\n",
    "                global_optim.step()\n",
    "\n",
    "                train_l_sum += l.item()\n",
    "                train_batch_count += 1\n",
    "            \n",
    "            if (epoch + 1) % print_frequency == 0:\n",
    "                with torch.no_grad():\n",
    "                    for x in align_testloader:\n",
    "                        concept_1 = x[0].to(device)\n",
    "                        # concept_1 += concept_eps * torch.rand_like(concept_1).to(device)\n",
    "                        concept_2 = x[1].to(device)\n",
    "                        # concept_2 += concept_eps * torch.rand_like(concept_2).to(device)\n",
    "                        translater_2to1.eval()\n",
    "                        concept_1_recons = translater_2to1(concept_2)\n",
    "                        translater_2to1.train()\n",
    "                        l = recons_criterion(concept_1_recons, concept_1)\n",
    "                        test_l_sum += l.item()\n",
    "                        test_batch_count += 1\n",
    "\n",
    "                print(\n",
    "                    f'epoch {epoch + 1},' \n",
    "                    f'align training loss {train_l_sum / train_batch_count:.4f},' \n",
    "                    f'align testing loss {test_l_sum / test_batch_count:.4f}'\n",
    "                )\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_agent_maze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
