{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. INIT - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math as mt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pympler import asizeof\n",
    "from visdom import Visdom\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import networkx as nx\n",
    "#this is set for the printing of Q-matrices via console\n",
    "torch.set_printoptions(precision=3, sci_mode=False, linewidth=100)\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#suppress scientific notation in printouts\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. INIT - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_color_hex: str = \"#2f78b6\" # hex color of init point (left top)\n",
    "goal_color_hex: str = '#f0b226' # hex color of goal point (right bottom)\n",
    "wall_color: list[float] = [0.7, 0.7, 0.7] # RGB color of walls\n",
    "\n",
    "n_cell: int = 4 # how many original cells\n",
    "grid_dim = n_cell * 2 - 1 # side length of the square gridworld, in the form of 2n-1\n",
    "n_actions: int = 4 # how many actions are possible in each state\n",
    "lava: bool = False #do we use lava states - i.e. accessible wall states - (True) or wall states (False)? (we now always use \"False\")\n",
    "starting_state: int = 0\n",
    "goal_state = grid_dim ** 2 - 1\n",
    "\n",
    "# generation of maze\n",
    "maze_gen: bool = False # generate wall_states? wall_state_dict {0: [], 1: [1], 2: [2], 3: [3], ... } consists of \n",
    "n_mazes: int = 3000\n",
    "\n",
    "# rewards in the gridworld (-0.1, 100, -5)\n",
    "step_reward: float = -0.1 # for taking a step\n",
    "goal_reward: float = 2. # for reaching the goal\n",
    "wall_reward: float = -1. # for bumping into a wall\n",
    "\n",
    "# CA-TS Net settings\n",
    "input_neurons: int = 2 # for network init\n",
    "output_neurons = n_actions # modeling the Q(s, a) value by the output neurons w.r.t. number of action\n",
    "concept_size: int = 64 # the concept vector size of CA-TS DQN\n",
    "using_concept_eps: bool = True\n",
    "concept_eps: float = 0.1\n",
    "concept_eps_str = f\"_cpteps{concept_eps}\" if using_concept_eps else \"\"\n",
    "using_res: bool = True\n",
    "res_str = \"_res\" if using_res else \"\"\n",
    "hidden_dims: list = [768] * 16\n",
    "dim_str = \"-\".join(str(d) for d in hidden_dims)\n",
    "q_s2a: bool = False # whether using Q(s) -> a: True or Q(s, a): False\n",
    "q_str = \"\" if q_s2a else \"_sa2q\"\n",
    "n_agents: int = 5 # how many anegts used in communication games\n",
    "n_unique: int = 20 # unique mazes learned by each agent\n",
    "n_group: int = 7\n",
    "\n",
    "# CA-TS Net training settings\n",
    "batch_size: int = 512 # 0 indicates using all data in buffer as a batch\n",
    "epsilon: float = 0.1 # greedy action policy\n",
    "lr: float = 1e-4 # learning rate\n",
    "gamma_bellman: float = 0.9999 # bellman equation\n",
    "target_replace_steps: int = 0 # renew target_net by eval_net after how many iter times, 0 indicates directly using eval_net as target_net\n",
    "memory_capacity: int = 0 # number transitions stored, 0 indicates pre-store all transitions in memory (change training mode as epoch manner)\n",
    "cap_str = \"\" if memory_capacity != 0 else \"_prestore\"\n",
    "n_episode: int = 100000\n",
    "\n",
    "# AE Net settings\n",
    "n_recons_test: int = 20\n",
    "n_align_test: int = 20\n",
    "ae_dims: list = [128, 192, 256, 192, 128]\n",
    "ae_dim_str = \"-\".join(str(d) for d in ae_dims)\n",
    "\n",
    "# AE Net training settings\n",
    "ae_batch_size: int = 128\n",
    "ae_weight_decay: float = 1e-5\n",
    "ae_align_lr: float = 1e-4\n",
    "align_weight: float = 1.0\n",
    "using_ae_eps: bool = False\n",
    "n_epoch: int = 10000\n",
    "print_frequency: int = 100\n",
    "train_ae: bool = True\n",
    "load_ae: bool = False\n",
    "loss_type: str = 'L1Loss' # options['MSELoss', 'L1Loss', 'SmoothL1Loss']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. FUNCTIONS - CA-TS Net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_sa2q(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_sa2q, self).__init__()\n",
    "        self.using_res = using_res\n",
    "\n",
    "        self.ts_fc_layers = nn.ModuleDict()\n",
    "        self.ts_norm_layers = nn.ModuleDict()\n",
    "        if self.using_res:\n",
    "            self.ts_skip_layers = nn.ModuleDict()\n",
    "\n",
    "        prev_dim = input_neurons + n_actions # dim 0, 1 is xy coordinates, dim 2 to 5 is action from 0 to 3 (right, uo, left, down)\n",
    "        for i, dim in enumerate(hidden_dims):\n",
    "            self.ts_fc_layers[f'fc{i}'] = nn.Linear(prev_dim, dim, bias=True)\n",
    "            if self.using_res and prev_dim != dim:\n",
    "                self.ts_skip_layers[f'skip{i}'] = nn.Linear(prev_dim, dim, bias=False)\n",
    "            self.ts_norm_layers[f'norm{i}'] = nn.LayerNorm(dim)\n",
    "            prev_dim = dim\n",
    "            \n",
    "\n",
    "        self.ts_fc_layers[f'fc{len(hidden_dims)}'] = nn.Linear(prev_dim, 1, bias=True)\n",
    "\n",
    "        self.cdp_fc_layers = nn.ModuleDict()\n",
    "        self.cdp_norm_layers = nn.ModuleDict()\n",
    "        if self.using_res:\n",
    "            self.cdp_skip_layers = nn.ModuleDict()\n",
    "\n",
    "        prev_dim = concept_size\n",
    "        for i, dim in enumerate(hidden_dims):\n",
    "            self.cdp_fc_layers[f'fc{i}'] = nn.Linear(prev_dim, dim, bias=True)\n",
    "            if self.using_res and prev_dim != dim:\n",
    "                self.cdp_skip_layers[f'skip{i}'] = nn.Linear(prev_dim, dim, bias=False)\n",
    "            self.cdp_norm_layers[f'norm{i}'] = nn.LayerNorm(dim)\n",
    "            prev_dim = dim\n",
    "\n",
    "        self.ts_afun = nn.ReLU()\n",
    "        self.cdp_afun = nn.Sigmoid()\n",
    "\n",
    "        self.concept_embedding_layer = nn.Embedding(num_embeddings=n_mazes, embedding_dim=concept_size)\n",
    "\n",
    "    def forward(self, x, concept_idx=None):\n",
    "        if concept_idx is not None:\n",
    "            concept = self.concept_embedding_layer(concept_idx)\n",
    "            cdp_activations = []\n",
    "            c = concept\n",
    "            for i in range(len(self.cdp_fc_layers)):\n",
    "                if self.using_res:\n",
    "                    identity = c\n",
    "                    out = self.cdp_fc_layers[f'fc{i}'](c)\n",
    "                    out = self.cdp_norm_layers[f'norm{i}'](out)\n",
    "                    if f'skip{i}' in self.cdp_skip_layers:\n",
    "                        identity = self.cdp_skip_layers[f'skip{i}'](identity)\n",
    "                    c = out + identity\n",
    "                else:\n",
    "                    c = self.cdp_fc_layers[f'fc{i}'](c)\n",
    "                    c = self.cdp_norm_layers[f'norm{i}'](c)\n",
    "\n",
    "                c = self.cdp_afun(c)\n",
    "                cdp_activations.append(c)\n",
    "\n",
    "        for i in range(len(self.ts_fc_layers) - 1):\n",
    "            if self.using_res:\n",
    "                identity = x\n",
    "                out = self.ts_fc_layers[f'fc{i}'](x)\n",
    "                out = self.ts_norm_layers[f'norm{i}'](out)\n",
    "                if f'skip{i}' in self.ts_skip_layers:\n",
    "                    identity = self.ts_skip_layers[f'skip{i}'](identity)\n",
    "                x = out + identity\n",
    "            else:\n",
    "                x = self.ts_fc_layers[f'fc{i}'](x)\n",
    "                x = self.ts_norm_layers[f'norm{i}'](x)\n",
    "                \n",
    "            x = self.ts_afun(x)\n",
    "\n",
    "            if concept_idx is not None:\n",
    "                x = torch.mul(x, cdp_activations[i])\n",
    "\n",
    "        x = self.ts_fc_layers[f'fc{len(self.cdp_fc_layers)}'](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. FUNCTIONS - Recons datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconsDataset(Dataset):\n",
    "    def __init__(self, concept_data):\n",
    "        self.concept_data = concept_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.concept_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.concept_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. FUNCTIONS - Alignment datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignDataset(Dataset):\n",
    "    def __init__(self, concept_data_list):\n",
    "        self.n_agents = len(concept_data_list)\n",
    "        self.concept_data_list = concept_data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.concept_data_list[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        grouped_item = []\n",
    "        for i in range(self.n_agents):\n",
    "            grouped_item.append(self.concept_data_list[i][idx])\n",
    "        return grouped_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. FUNCTIONS - Processing exclude mazes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recons_indices(n_mazes, n_agents, i_agent, n_unique, n_recons_test):\n",
    "    common_end = n_mazes - n_agents * n_unique\n",
    "    indices_list1 = list(range(0, common_end))\n",
    "    \n",
    "    unique_start = common_end + (i_agent - 1) * n_unique\n",
    "    unique_end = unique_start + n_unique\n",
    "    indices_list2 = list(range(unique_start, unique_end))\n",
    "\n",
    "    combined_list = indices_list1 + indices_list2\n",
    "    test_indices = random.sample(combined_list, n_recons_test)\n",
    "    train_indices = [item for item in combined_list if item not in test_indices]\n",
    "\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_align_indices(n_mazes, n_agents, n_unique, n_align_test):\n",
    "    common_end = n_mazes - n_agents * n_unique\n",
    "    indices_list = list(range(0, common_end))\n",
    "\n",
    "    test_indices = random.sample(indices_list, n_align_test)\n",
    "    train_indices = [item for item in indices_list if item not in test_indices]\n",
    "\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_indices(n_mazes, n_agents, i_agent, n_unique):\n",
    "    common_end = n_mazes - n_agents * n_unique\n",
    "    unique_start = common_end + (i_agent - 1) * n_unique\n",
    "    unique_end = unique_start + n_unique\n",
    "\n",
    "    return list(range(unique_start, unique_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_indices(n_mazes, n_agents, n_unique):\n",
    "    common_end = n_mazes - n_agents * n_unique\n",
    "\n",
    "    return list(range(0, common_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. FUNCTIONS - Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        mid_index = len(ae_dims) // 2\n",
    "        # encoder from concept_size to bottleneck\n",
    "        encoder_layers = []\n",
    "        prev_dim = concept_size\n",
    "        for i in range(mid_index + 1):  # include bottleneck\n",
    "            encoder_layers.append(nn.Linear(prev_dim, ae_dims[i]))\n",
    "            # add ReLU except bottleneck\n",
    "            if i != mid_index:\n",
    "                encoder_layers.append(nn.LayerNorm(ae_dims[i]))\n",
    "                encoder_layers.append(nn.ReLU())\n",
    "            prev_dim = ae_dims[i]\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # decoder from bottleneck to concept_size\n",
    "        decoder_layers = []\n",
    "        prev_dim = ae_dims[mid_index]  # latent dim\n",
    "        for i in range(mid_index - 1, -1, -1):\n",
    "            decoder_layers.append(nn.Linear(prev_dim, ae_dims[i]))\n",
    "            decoder_layers.append(nn.LayerNorm(ae_dims[i]))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "            prev_dim = ae_dims[i]\n",
    "        # last layer, no ReLU\n",
    "        decoder_layers.append(nn.Linear(prev_dim, concept_size))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "        self.t = nn.Parameter(torch.tensor(1.0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "    \n",
    "    def encode(self, x):\n",
    "        if using_ae_eps:\n",
    "            x = x + concept_eps * torch.rand_like(x).to(device)\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        return self.decoder(latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. FUNCTIONS - Test cross-agent concept reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_alignment(agents_ae, align_testloader, recons_criterion, agent_i = None, agent_j = None):\n",
    "    # Get a batch of aligned concepts from the test set\n",
    "    test_batch = next(iter(align_testloader))\n",
    "    \n",
    "    if agent_i is None or agent_j is None:\n",
    "        agent_indices = list(range(1, n_agents + 1))\n",
    "        agent_i, agent_j = random.sample(agent_indices, 2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get agent1's concept vectors\n",
    "        agenti_concepts = test_batch[agent_i - 1].to(device)\n",
    "        agentj_concepts = test_batch[agent_j - 1].to(device)\n",
    "\n",
    "        agents_ae[agent_i].eval()\n",
    "        agents_ae[agent_j].eval()\n",
    "        # Encode using agent1's autoencoder\n",
    "        latent_agenti = agents_ae[agent_i].encode(agenti_concepts)\n",
    "        latent_agentj = agents_ae[agent_j].encode(agentj_concepts)\n",
    "\n",
    "        # Decode using agent2's autoencoder\n",
    "        recons_agentjbylatenti = agents_ae[agent_j].decode(latent_agenti)\n",
    "        recons_agentjbylatentj = agents_ae[agent_j].decode(latent_agentj)\n",
    "        recons_agentibylatentj = agents_ae[agent_i].decode(latent_agentj)\n",
    "        recons_agentibylatenti = agents_ae[agent_i].decode(latent_agenti)\n",
    "\n",
    "        agents_ae[agent_i].train()\n",
    "        agents_ae[agent_j].train()\n",
    "\n",
    "        # Calculate MSE between reconstructed and original\n",
    "        recons_loss1 = recons_criterion(recons_agentibylatentj, agenti_concepts)\n",
    "        recons_loss2 = recons_criterion(recons_agentjbylatentj, agentj_concepts)\n",
    "        recons_loss3 = recons_criterion(recons_agentjbylatenti, agentj_concepts)\n",
    "        recons_loss4 = recons_criterion(recons_agentibylatenti, agenti_concepts)\n",
    "\n",
    "        latent_loss1 = recons_criterion(latent_agenti, latent_agentj)\n",
    "        latent_loss2 = recons_criterion(latent_agenti[0:-2], latent_agentj[1:-1])\n",
    "\n",
    "    print(f\"Loss between same latents of agent{agent_i}'s and agent{agent_j}'s: {latent_loss1.item():.4f}\")\n",
    "    print(f\"Loss between different latents of agent{agent_i}'s and agent{agent_j}'s: {latent_loss2.item():.4f}\")\n",
    "    print(f\"Loss between agent{agent_i}'s original concepts and agent{agent_j}'s latent reconstructed in agent{agent_i}'s space: {recons_loss1.item():.4f}\")\n",
    "    print(f\"Loss between agent{agent_j}'s original concepts and agent{agent_j}'s latent reconstructed in agent{agent_j}'s space: {recons_loss2.item():.4f}\")\n",
    "    print(f\"Loss between agent{agent_j}'s original concepts and agent{agent_i}'s latent reconstructed in agent{agent_j}'s space: {recons_loss3.item():.4f}\")\n",
    "    print(f\"Loss between agent{agent_i}'s original concepts and agent{agent_i}'s latent reconstructed in agent{agent_i}'s space: {recons_loss4.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. FUNCTIONS - Alignment contrast loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_clip_loss(latent_i, latent_j, t = torch.tensor(0.0)):\n",
    "    normalized_latent_i = torch.nn.functional.normalize(latent_i, p = 2, dim = 1)\n",
    "    normalized_latent_j = torch.nn.functional.normalize(latent_j, p = 2, dim = 1)\n",
    "    logits = torch.matmul(normalized_latent_i, normalized_latent_j.T) * torch.exp(t)\n",
    "    labels = torch.arange(latent_i.size(0)).to(device)\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    loss_j = torch.nn.functional.cross_entropy(logits.T, labels)\n",
    "    return (loss_i + loss_j) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_clip_l1loss(latent_i, latent_j):\n",
    "    batch_size = latent_i.size(0)\n",
    "    hidden_dim = latent_i.size(1)\n",
    "    \n",
    "    distance_matrix = torch.cdist(latent_i, latent_j, p=1) / hidden_dim\n",
    "    \n",
    "    # Create mask for diagonal elements\n",
    "    diag_mask = torch.eye(batch_size, dtype=torch.bool, device=device)\n",
    "    \n",
    "    # Calculate diagonal loss\n",
    "    diag_loss = torch.sum(distance_matrix[diag_mask])\n",
    "    \n",
    "    # Calculate off-diagonal loss with margin\n",
    "    off_diag_mask = ~diag_mask\n",
    "    margin = torch.tensor(0.3, device=device)\n",
    "    off_diag_loss = torch.sum(torch.clamp_min(margin - distance_matrix[off_diag_mask], 0))\n",
    "    \n",
    "    total_loss = diag_loss + off_diag_loss\n",
    "    return total_loss / (batch_size * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_balanced_l1loss(latent_i, latent_j):\n",
    "    batch_size = latent_i.size(0)\n",
    "    hidden_dim = latent_i.size(1)\n",
    "    \n",
    "    # Part 1: Direct L1 loss between latent_i and latent_j\n",
    "    direct_loss = torch.nn.functional.l1_loss(latent_i, latent_j)\n",
    "    \n",
    "    # Part 2: Shift latent_j indices and compute L1 loss\n",
    "    # Create shifted indices using modulo operation\n",
    "    shifted_indices = (torch.arange(batch_size, device=device) + torch.randint(low=1, high=batch_size, size=(), device=device)) % batch_size\n",
    "    latent_j_shifted = latent_j[shifted_indices]\n",
    "\n",
    "    # Compute L1 loss between latent_i and shifted latent_j\n",
    "    margin = torch.tensor(0.3, device=device)\n",
    "    shifted_loss = torch.clamp_min(margin - torch.nn.functional.l1_loss(latent_i, latent_j_shifted), 0)\n",
    "    \n",
    "    # Combine both losses with equal weight\n",
    "    total_loss = (direct_loss + shifted_loss) * 0.5\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. EXECUTION - Constructing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = {}\n",
    "for i in range(n_agents):\n",
    "    i_agent = i + 1\n",
    "    agents[i_agent] = Net_sa2q()\n",
    "    ckpt_file: str = f\"agent_ckpt/group{n_group}/ckpt{i_agent}of{n_agents}_unique{n_unique}_{grid_dim}x{grid_dim}_n{n_mazes}{q_str}{cap_str}_dim{dim_str}{res_str}_cptsz{concept_size}{concept_eps_str}_lr{lr}_epsi{n_episode}_gamma{gamma_bellman}_bs{batch_size}_tr{target_replace_steps}.pt\"\n",
    "    agents[i_agent].load_state_dict(torch.load(ckpt_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_trainsets = {}\n",
    "recons_trainloaders = {}\n",
    "recons_testsets = {}\n",
    "recons_testloaders = {}\n",
    "\n",
    "for i in range(n_agents):\n",
    "    i_agent = i + 1\n",
    "    recons_train_indices, recons_test_indices = get_recons_indices(n_mazes, n_agents, i_agent, n_unique, n_recons_test)\n",
    "    recons_trainsets[i_agent] = ReconsDataset(agents[i_agent].concept_embedding_layer.weight.data[recons_train_indices])\n",
    "    recons_testsets[i_agent] = ReconsDataset(agents[i_agent].concept_embedding_layer.weight.data[recons_test_indices])\n",
    "\n",
    "    recons_trainloaders[i_agent] = DataLoader(recons_trainsets[i_agent], batch_size = ae_batch_size, shuffle = True)\n",
    "    recons_testloaders[i_agent] = DataLoader(recons_testsets[i_agent], batch_size = ae_batch_size, shuffle = False)\n",
    "\n",
    "align_train_indices, align_test_indices = get_align_indices(n_mazes, n_agents, n_unique, n_align_test)\n",
    "align_train_concept = []\n",
    "align_test_concept = []\n",
    "for i in range(n_agents):\n",
    "    i_agent = i + 1\n",
    "    align_train_concept.append(agents[i_agent].concept_embedding_layer.weight.data[align_train_indices])\n",
    "    align_test_concept.append(agents[i_agent].concept_embedding_layer.weight.data[align_test_indices])\n",
    "\n",
    "align_trainset = AlignDataset(align_train_concept)\n",
    "align_testset = AlignDataset(align_test_concept)\n",
    "\n",
    "align_trainloader = DataLoader(align_trainset, batch_size = ae_batch_size, shuffle = True)\n",
    "align_testloader = DataLoader(align_testset, batch_size = ae_batch_size, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. EXECUTION - Two training stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method 1: odd-numbered epochs, self-reconstruction; even-numbered epochs, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents_ae = {}\n",
    "# for i in range(n_agents):\n",
    "#     i_agent = i + 1\n",
    "#     agents_ae[i_agent] = Autoencoder().to(device)\n",
    "#     if load_ae:\n",
    "#         ae_ckpt_file: str = f\"ae_ckpt/group{n_group}_dim{ae_dim_str}_ReconsTest{n_recons_test}_AlignTest{n_align_test}_bs{ae_batch_size}_AlignLr{ae_align_lr}_epo{n_epoch}_{loss_type}/ae{i_agent}of{n_agents}.pt\"\n",
    "#         agents_ae[i_agent].load_state_dict(torch.load(ae_ckpt_file))\n",
    "#         print(f'Autoencoder of agent {i_agent} loaded!')\n",
    "\n",
    "# global_optim = torch.optim.Adam([p for model in agents_ae.values() for p in model.parameters()], lr = ae_align_lr, weight_decay = ae_weight_decay)\n",
    "\n",
    "# recons_criterion = nn.__dict__[loss_type]()\n",
    "\n",
    "# if train_ae:\n",
    "#     try:\n",
    "#         for epoch in range(n_epoch):\n",
    "#             if epoch % 2 == 0:\n",
    "#                 for i in range(n_agents):\n",
    "#                     i_agent = i + 1\n",
    "#                     train_l_sum = 0.\n",
    "#                     train_batch_count = 0\n",
    "#                     test_l_sum = 0.\n",
    "#                     test_batch_count = 0\n",
    "#                     for x in recons_trainloaders[i_agent]:\n",
    "#                         x = x.to(device)\n",
    "#                         x_hat = agents_ae[i_agent](x)\n",
    "#                         recons_loss = recons_criterion(x_hat, x)\n",
    "\n",
    "#                         global_optim.zero_grad()\n",
    "#                         recons_loss.backward()\n",
    "#                         global_optim.step()\n",
    "\n",
    "#                         train_l_sum += recons_loss.cpu().item()\n",
    "#                         train_batch_count += 1\n",
    "                    \n",
    "#                     for x in recons_testloaders[i_agent]:\n",
    "#                         x = x.to(device)\n",
    "#                         with torch.no_grad():\n",
    "#                             agents_ae[i_agent].eval()\n",
    "#                             x_hat = agents_ae[i_agent](x)\n",
    "#                             agents_ae[i_agent].train()\n",
    "#                             recons_loss = recons_criterion(x_hat, x)\n",
    "\n",
    "#                         test_l_sum += recons_loss.cpu().item()\n",
    "#                         test_batch_count += 1\n",
    "                    \n",
    "#                     if (epoch) % print_frequency == 0:\n",
    "#                         print(f'epoch {epoch + 1}, agent {i_agent}, recons training loss {train_l_sum / train_batch_count:.4f}, recons testing loss {test_l_sum / test_batch_count:.4f}')\n",
    "                \n",
    "#                 # pass\n",
    "\n",
    "#             else:\n",
    "#                 train_l_sum = 0.\n",
    "#                 train_batch_count = 0\n",
    "#                 test_l_sum = 0.\n",
    "#                 test_l_sum_recons = 0.\n",
    "#                 test_l_sum_align = 0.\n",
    "#                 test_batch_count = 0\n",
    "#                 for x in align_trainloader:\n",
    "#                     latent = []\n",
    "#                     for i in range(n_agents):\n",
    "#                         i_agent = i + 1\n",
    "#                         x_in = x[i].to(device)\n",
    "#                         x_hat = agents_ae[i_agent].encode(x_in)\n",
    "#                         latent.append(x_hat)\n",
    "\n",
    "#                     # # method 1: mean of all latent vectors\n",
    "#                     # stacked_latent = torch.stack(latent)  # shape (n_agents, batch_size, dim_latent)\n",
    "#                     # mean_latent = stacked_latent.mean(dim = 0, keepdim = True).repeat(n_agents, 1, 1)\n",
    "#                     # align_loss = F.mse_loss(stacked_latent, mean_latent)\n",
    "\n",
    "#                     # method 2: align between each latent vector plus mutual reconstruction loss\n",
    "#                     align_loss = 0.0\n",
    "#                     for i in range(n_agents):\n",
    "#                         for j in range(i+1, n_agents):\n",
    "#                             align_loss += align_weight * recons_criterion(latent[i], latent[j])\n",
    "\n",
    "#                     recon_loss = 0.0\n",
    "#                     for i in range(n_agents):\n",
    "#                         x_in = x[i].to(device)\n",
    "#                         for j in range(n_agents):\n",
    "#                             # if i != j:\n",
    "#                                 x_tar = x[j].to(device)\n",
    "#                                 j_agent = j + 1\n",
    "#                                 recon_loss += recons_criterion(agents_ae[j_agent].decode(latent[i]), x_tar)\n",
    "\n",
    "#                     align_loss = align_loss + recon_loss\n",
    "\n",
    "#                     global_optim.zero_grad()\n",
    "#                     align_loss.backward()\n",
    "#                     global_optim.step()\n",
    "\n",
    "#                     train_l_sum += align_loss.cpu().item()\n",
    "#                     train_batch_count += 1\n",
    "\n",
    "#                 with torch.no_grad():\n",
    "#                     for x in align_testloader:\n",
    "#                         latent = []\n",
    "#                         for i in range(n_agents):\n",
    "#                             i_agent = i + 1\n",
    "#                             agents_ae[i_agent].eval()\n",
    "#                             x_in = x[i].to(device)\n",
    "#                             x_hat = agents_ae[i_agent].encode(x_in)\n",
    "#                             latent.append(x_hat)\n",
    "#                             agents_ae[i_agent].train()\n",
    "\n",
    "#                         # # method 1: mean of all latent vectors\n",
    "#                         # stacked_latent = torch.stack(latent)  # shape (n_agents, batch_size, dim_latent)\n",
    "#                         # mean_latent = stacked_latent.mean(dim = 0, keepdim = True).repeat(n_agents, 1, 1)\n",
    "#                         # align_loss = F.mse_loss(stacked_latent, mean_latent)\n",
    "\n",
    "#                         # method 2: align between each latent vector plus mutual reconstruction loss\n",
    "#                         align_loss = 0.0\n",
    "#                         for i in range(n_agents):\n",
    "#                             for j in range(i+1, n_agents):\n",
    "#                                 align_loss += align_weight * recons_criterion(latent[i], latent[j])\n",
    "#                         test_l_sum_align += align_loss.cpu().item()\n",
    "\n",
    "#                         recon_loss = 0.0\n",
    "#                         for i in range(n_agents):\n",
    "#                             x_in = x[i].to(device)\n",
    "#                             for j in range(n_agents):\n",
    "#                                 # if i != j:\n",
    "#                                     x_tar = x[j].to(device)\n",
    "#                                     j_agent = j + 1\n",
    "#                                     agents_ae[j_agent].eval()\n",
    "#                                     recon_loss += recons_criterion(agents_ae[j_agent].decode(latent[i]), x_tar) # was set to x_in before\n",
    "#                                     agents_ae[j_agent].train()\n",
    "#                         test_l_sum_recons += recon_loss.cpu().item()\n",
    "                        \n",
    "#                         align_loss = align_loss + recon_loss\n",
    "\n",
    "#                         test_l_sum += align_loss.cpu().item()\n",
    "#                         test_batch_count += 1\n",
    "\n",
    "#                 if (epoch + 1) % print_frequency == 0:\n",
    "#                     print(\n",
    "#                         f'epoch {epoch + 1},' \n",
    "#                         f'align training loss {train_l_sum / train_batch_count:.4f},' \n",
    "#                         f'align sub-recons testing loss {test_l_sum_recons / test_batch_count:.4f},'\n",
    "#                         f'align sub-align testing loss {test_l_sum_align / test_batch_count:.4f},'\n",
    "#                         f'align testing loss {test_l_sum / test_batch_count:.4f}'\n",
    "#                     )\n",
    "#                     print('On training set alignment...')\n",
    "#                     test_alignment(agents_ae, align_trainloader, recons_criterion)\n",
    "#                     print('On testing set alignment...')\n",
    "#                     test_alignment(agents_ae, align_testloader, recons_criterion)\n",
    "    \n",
    "#     except KeyboardInterrupt:\n",
    "#         pass\n",
    "\n",
    "#     for i in range(n_agents):\n",
    "#         i_agent = i + 1\n",
    "#         ae_ckpt_file: str = f\"ae_ckpt/group{n_group}_dim{ae_dim_str}_ReconsTest{n_recons_test}_AlignTest{n_align_test}_bs{ae_batch_size}_AlignLr{ae_align_lr}_epo{epoch+1}_{loss_type}/ae{i_agent}of{n_agents}.pt\"\n",
    "        \n",
    "#         os.makedirs(os.path.dirname(ae_ckpt_file), exist_ok=True)\n",
    "\n",
    "#         torch.save(agents_ae[i_agent].state_dict(), ae_ckpt_file)\n",
    "#         print(f'Autoencoder of agent {i_agent} saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method 2: encoder alignment until convergence, then training decoder reconstruction using fixed encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents_ae = {}\n",
    "# for i in range(n_agents):\n",
    "#     i_agent = i + 1\n",
    "#     agents_ae[i_agent] = Autoencoder().to(device)\n",
    "#     if load_ae:\n",
    "#         ae_ckpt_file: str = f\"ae_ckpt/group{n_group}_dim{ae_dim_str}_ReconsTest{n_recons_test}_AlignTest{n_align_test}_bs{ae_batch_size}_AlignLr{ae_align_lr}_epo{n_epoch}_{loss_type}/ae{i_agent}of{n_agents}.pt\"\n",
    "#         agents_ae[i_agent].load_state_dict(torch.load(ae_ckpt_file))\n",
    "#         print(f'Autoencoder of agent {i_agent} loaded!')\n",
    "\n",
    "# global_optim = torch.optim.Adam([p for model in agents_ae.values() for p in model.parameters()], lr = ae_align_lr, weight_decay = ae_weight_decay)\n",
    "\n",
    "# recons_criterion = nn.__dict__[loss_type]()\n",
    "\n",
    "# if train_ae:\n",
    "#     try:\n",
    "#         for epoch in range(n_epoch):\n",
    "#             if epoch < 1000:\n",
    "#                 train_l_sum = 0.\n",
    "#                 train_batch_count = 0\n",
    "#                 test_l_sum = 0.\n",
    "#                 test_l_sum_recons = 0.\n",
    "#                 test_l_sum_align = 0.\n",
    "#                 test_batch_count = 0\n",
    "#                 for x in align_trainloader:\n",
    "#                     latent = []\n",
    "#                     for i in range(n_agents):\n",
    "#                         i_agent = i + 1\n",
    "#                         x_in = x[i].to(device)\n",
    "#                         x_hat = agents_ae[i_agent].encode(x_in)\n",
    "#                         latent.append(x_hat)\n",
    "\n",
    "#                     # # method 1: mean of all latent vectors\n",
    "#                     # stacked_latent = torch.stack(latent)  # shape (n_agents, batch_size, dim_latent)\n",
    "#                     # mean_latent = stacked_latent.mean(dim = 0, keepdim = True).repeat(n_agents, 1, 1)\n",
    "#                     # align_loss = F.mse_loss(stacked_latent, mean_latent)\n",
    "\n",
    "#                     # method 2: align between each latent vector plus mutual reconstruction loss\n",
    "#                     align_loss = 0.0\n",
    "#                     for i in range(n_agents):\n",
    "#                         for j in range(i+1, n_agents):\n",
    "#                             align_loss += align_weight * recons_criterion(latent[i], latent[j])\n",
    "\n",
    "#                     global_optim.zero_grad()\n",
    "#                     align_loss.backward()\n",
    "#                     global_optim.step()\n",
    "\n",
    "#                     train_l_sum += align_loss.cpu().item()\n",
    "#                     train_batch_count += 1\n",
    "\n",
    "#                 if (epoch + 1) % print_frequency == 0:\n",
    "#                     print(\n",
    "#                         f'epoch {epoch + 1},' \n",
    "#                         f'align training loss {train_l_sum / train_batch_count:.4f},' \n",
    "#                     )\n",
    "#                     print('On training set alignment...')\n",
    "#                     test_alignment(agents_ae, align_trainloader, recons_criterion)\n",
    "#                     print('On testing set alignment...')\n",
    "#                     test_alignment(agents_ae, align_testloader, recons_criterion)\n",
    "\n",
    "#             else:\n",
    "#                 train_l_sum = 0.\n",
    "#                 train_batch_count = 0\n",
    "#                 test_l_sum = 0.\n",
    "#                 test_l_sum_recons = 0.\n",
    "#                 test_l_sum_align = 0.\n",
    "#                 test_batch_count = 0\n",
    "#                 for x in align_trainloader:\n",
    "#                     latent = []\n",
    "#                     with torch.no_grad():\n",
    "#                         for i in range(n_agents):\n",
    "#                             i_agent = i + 1\n",
    "#                             x_in = x[i].to(device)\n",
    "#                             x_hat = agents_ae[i_agent].encode(x_in)\n",
    "#                             latent.append(x_hat)\n",
    "\n",
    "#                     # # method 1: mean of all latent vectors\n",
    "#                     # stacked_latent = torch.stack(latent)  # shape (n_agents, batch_size, dim_latent)\n",
    "#                     # mean_latent = stacked_latent.mean(dim = 0, keepdim = True).repeat(n_agents, 1, 1)\n",
    "#                     # align_loss = F.mse_loss(stacked_latent, mean_latent)\n",
    "\n",
    "#                     # method 2: align between each latent vector plus mutual reconstruction loss\n",
    "#                     align_loss = 0.0\n",
    "#                     for i in range(n_agents):\n",
    "#                         for j in range(i+1, n_agents):\n",
    "#                             align_loss += align_weight * recons_criterion(latent[i], latent[j])\n",
    "\n",
    "#                     recon_loss = 0.0\n",
    "#                     for i in range(n_agents):\n",
    "#                         x_in = x[i].to(device)\n",
    "#                         for j in range(n_agents):\n",
    "#                             # if i != j:\n",
    "#                                 x_tar = x[j].to(device)\n",
    "#                                 j_agent = j + 1\n",
    "#                                 recon_loss += recons_criterion(agents_ae[j_agent].decode(latent[i]), x_tar)\n",
    "\n",
    "#                     align_loss = align_loss + recon_loss\n",
    "\n",
    "#                     global_optim.zero_grad()\n",
    "#                     align_loss.backward()\n",
    "#                     global_optim.step()\n",
    "\n",
    "#                     train_l_sum += align_loss.cpu().item()\n",
    "#                     train_batch_count += 1\n",
    "                    \n",
    "#                 if (epoch + 1) % print_frequency == 0:\n",
    "#                     with torch.no_grad():\n",
    "#                         for x in align_testloader:\n",
    "#                             latent = []\n",
    "#                             for i in range(n_agents):\n",
    "#                                 i_agent = i + 1\n",
    "#                                 agents_ae[i_agent].eval()\n",
    "#                                 x_in = x[i].to(device)\n",
    "#                                 x_hat = agents_ae[i_agent].encode(x_in)\n",
    "#                                 latent.append(x_hat)\n",
    "#                                 agents_ae[i_agent].train()\n",
    "\n",
    "#                             # # method 1: mean of all latent vectors\n",
    "#                             # stacked_latent = torch.stack(latent)  # shape (n_agents, batch_size, dim_latent)\n",
    "#                             # mean_latent = stacked_latent.mean(dim = 0, keepdim = True).repeat(n_agents, 1, 1)\n",
    "#                             # align_loss = F.mse_loss(stacked_latent, mean_latent)\n",
    "\n",
    "#                             # method 2: align between each latent vector plus mutual reconstruction loss\n",
    "#                             align_loss = 0.0\n",
    "#                             for i in range(n_agents):\n",
    "#                                 for j in range(i+1, n_agents):\n",
    "#                                     align_loss += align_weight * recons_criterion(latent[i], latent[j])\n",
    "#                             test_l_sum_align += align_loss.cpu().item()\n",
    "\n",
    "#                             recon_loss = 0.0\n",
    "#                             for i in range(n_agents):\n",
    "#                                 x_in = x[i].to(device)\n",
    "#                                 for j in range(n_agents):\n",
    "#                                     # if i != j:\n",
    "#                                         x_tar = x[j].to(device)\n",
    "#                                         j_agent = j + 1\n",
    "#                                         agents_ae[j_agent].eval()\n",
    "#                                         recon_loss += recons_criterion(agents_ae[j_agent].decode(latent[i]), x_tar) # was set to x_in before\n",
    "#                                         agents_ae[j_agent].train()\n",
    "#                             test_l_sum_recons += recon_loss.cpu().item()\n",
    "                            \n",
    "#                             align_loss = align_loss + recon_loss\n",
    "\n",
    "#                             test_l_sum += align_loss.cpu().item()\n",
    "#                             test_batch_count += 1\n",
    "\n",
    "#                     print(\n",
    "#                         f'epoch {epoch + 1},' \n",
    "#                         f'align training loss {train_l_sum / train_batch_count:.4f},' \n",
    "#                         f'align sub-recons testing loss {test_l_sum_recons / test_batch_count:.4f},'\n",
    "#                         f'align sub-align testing loss {test_l_sum_align / test_batch_count:.4f},'\n",
    "#                         f'align testing loss {test_l_sum / test_batch_count:.4f}'\n",
    "#                     )\n",
    "#                     print('On training set alignment...')\n",
    "#                     test_alignment(agents_ae, align_trainloader, recons_criterion)\n",
    "#                     print('On testing set alignment...')\n",
    "#                     test_alignment(agents_ae, align_testloader, recons_criterion)\n",
    "    \n",
    "#     except KeyboardInterrupt:\n",
    "#         pass\n",
    "\n",
    "#     for i in range(n_agents):\n",
    "#         i_agent = i + 1\n",
    "#         ae_ckpt_file: str = f\"ae_ckpt/group{n_group}_dim{ae_dim_str}_ReconsTest{n_recons_test}_AlignTest{n_align_test}_bs{ae_batch_size}_AlignLr{ae_align_lr}_epo{epoch+1}_{loss_type}/ae{i_agent}of{n_agents}.pt\"\n",
    "        \n",
    "#         os.makedirs(os.path.dirname(ae_ckpt_file), exist_ok=True)\n",
    "\n",
    "#         torch.save(agents_ae[i_agent].state_dict(), ae_ckpt_file)\n",
    "#         print(f'Autoencoder of agent {i_agent} saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method 3: contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents_ae = {}\n",
    "# for i in range(n_agents):\n",
    "#     i_agent = i + 1\n",
    "#     agents_ae[i_agent] = Autoencoder().to(device)\n",
    "#     if load_ae:\n",
    "#         ae_ckpt_file: str = f\"ae_ckpt/group{n_group}_dim{ae_dim_str}_ReconsTest{n_recons_test}_AlignTest{n_align_test}_bs{ae_batch_size}_AlignLr{ae_align_lr}_epo{n_epoch}_{loss_type}/ae{i_agent}of{n_agents}.pt\"\n",
    "#         agents_ae[i_agent].load_state_dict(torch.load(ae_ckpt_file))\n",
    "#         print(f'Autoencoder of agent {i_agent} loaded!')\n",
    "\n",
    "# global_optim = torch.optim.Adam([p for model in agents_ae.values() for p in model.parameters()], lr = ae_align_lr, weight_decay = ae_weight_decay)\n",
    "\n",
    "# recons_criterion = nn.__dict__[loss_type]()\n",
    "\n",
    "# if train_ae:\n",
    "#     try:\n",
    "#         for epoch in range(n_epoch):\n",
    "#             if epoch % 2 == 0:\n",
    "#                 for i in range(n_agents):\n",
    "#                     i_agent = i + 1\n",
    "#                     train_l_sum = 0.\n",
    "#                     train_batch_count = 0\n",
    "#                     test_l_sum = 0.\n",
    "#                     test_batch_count = 0\n",
    "#                     for x in recons_trainloaders[i_agent]:\n",
    "#                         x = x.to(device)\n",
    "#                         x_hat = agents_ae[i_agent](x)\n",
    "#                         recons_loss = recons_criterion(x_hat, x)\n",
    "\n",
    "#                         global_optim.zero_grad()\n",
    "#                         recons_loss.backward()\n",
    "#                         global_optim.step()\n",
    "\n",
    "#                         train_l_sum += recons_loss.cpu().item()\n",
    "#                         train_batch_count += 1\n",
    "                    \n",
    "#                     for x in recons_testloaders[i_agent]:\n",
    "#                         x = x.to(device)\n",
    "#                         with torch.no_grad():\n",
    "#                             agents_ae[i_agent].eval()\n",
    "#                             x_hat = agents_ae[i_agent](x)\n",
    "#                             agents_ae[i_agent].train()\n",
    "#                             recons_loss = recons_criterion(x_hat, x)\n",
    "\n",
    "#                         test_l_sum += recons_loss.cpu().item()\n",
    "#                         test_batch_count += 1\n",
    "                    \n",
    "#                     if (epoch) % print_frequency == 0:\n",
    "#                         print(f'epoch {epoch + 1}, agent {i_agent}, recons training loss {train_l_sum / train_batch_count:.4f}, recons testing loss {test_l_sum / test_batch_count:.4f}')\n",
    "                \n",
    "#                 if (epoch) % print_frequency == 0:\n",
    "#                     print('On training set alignment...')\n",
    "#                     test_alignment(agents_ae, align_trainloader, recons_criterion)\n",
    "#                     print('On testing set alignment...')\n",
    "#                     test_alignment(agents_ae, align_testloader, recons_criterion)\n",
    "\n",
    "#                 pass\n",
    "\n",
    "#             else:\n",
    "#                 train_l_sum = 0.\n",
    "#                 train_batch_count = 0\n",
    "#                 test_l_sum = 0.\n",
    "#                 test_l_sum_recons = 0.\n",
    "#                 test_l_sum_align = 0.\n",
    "#                 test_batch_count = 0\n",
    "#                 for x in align_trainloader:\n",
    "#                     latent = []\n",
    "#                     for i in range(n_agents):\n",
    "#                         i_agent = i + 1\n",
    "#                         x_in = x[i].to(device)\n",
    "#                         x_hat = agents_ae[i_agent].encode(x_in)\n",
    "#                         latent.append(x_hat)\n",
    "\n",
    "#                     # # method 1: mean of all latent vectors\n",
    "#                     # stacked_latent = torch.stack(latent)  # shape (n_agents, batch_size, dim_latent)\n",
    "#                     # mean_latent = stacked_latent.mean(dim = 0, keepdim = True).repeat(n_agents, 1, 1)\n",
    "#                     # align_loss = F.mse_loss(stacked_latent, mean_latent)\n",
    "\n",
    "#                     # method 2: align between each latent vector plus mutual reconstruction loss\n",
    "#                     align_loss = 0.0\n",
    "#                     for i in range(n_agents):\n",
    "#                         for j in range(i+1, n_agents):\n",
    "#                             align_loss += align_weight * align_balanced_l1loss(latent[i], latent[j])\n",
    "\n",
    "#                     recon_loss = 0.0\n",
    "#                     for i in range(n_agents):\n",
    "#                         x_in = x[i].to(device)\n",
    "#                         for j in range(n_agents):\n",
    "#                             # if i != j:\n",
    "#                                 x_tar = x[j].to(device)\n",
    "#                                 j_agent = j + 1\n",
    "#                                 recon_loss += recons_criterion(agents_ae[j_agent].decode(latent[i]), x_tar)\n",
    "\n",
    "#                     align_loss = align_loss + recon_loss\n",
    "\n",
    "#                     global_optim.zero_grad()\n",
    "#                     align_loss.backward()\n",
    "#                     global_optim.step()\n",
    "\n",
    "#                     train_l_sum += align_loss.cpu().item()\n",
    "#                     train_batch_count += 1\n",
    "\n",
    "#                 with torch.no_grad():\n",
    "#                     for x in align_testloader:\n",
    "#                         latent = []\n",
    "#                         for i in range(n_agents):\n",
    "#                             i_agent = i + 1\n",
    "#                             agents_ae[i_agent].eval()\n",
    "#                             x_in = x[i].to(device)\n",
    "#                             x_hat = agents_ae[i_agent].encode(x_in)\n",
    "#                             latent.append(x_hat)\n",
    "#                             agents_ae[i_agent].train()\n",
    "\n",
    "#                         # # method 1: mean of all latent vectors\n",
    "#                         # stacked_latent = torch.stack(latent)  # shape (n_agents, batch_size, dim_latent)\n",
    "#                         # mean_latent = stacked_latent.mean(dim = 0, keepdim = True).repeat(n_agents, 1, 1)\n",
    "#                         # align_loss = F.mse_loss(stacked_latent, mean_latent)\n",
    "\n",
    "#                         # method 2: align between each latent vector plus mutual reconstruction loss\n",
    "#                         align_loss = 0.0\n",
    "#                         for i in range(n_agents):\n",
    "#                             for j in range(i+1, n_agents):\n",
    "#                                 align_loss += align_weight * align_balanced_l1loss(latent[i], latent[j])\n",
    "#                         test_l_sum_align += align_loss.cpu().item()\n",
    "\n",
    "#                         recon_loss = 0.0\n",
    "#                         for i in range(n_agents):\n",
    "#                             x_in = x[i].to(device)\n",
    "#                             for j in range(n_agents):\n",
    "#                                 # if i != j:\n",
    "#                                     x_tar = x[j].to(device)\n",
    "#                                     j_agent = j + 1\n",
    "#                                     agents_ae[j_agent].eval()\n",
    "#                                     recon_loss += recons_criterion(agents_ae[j_agent].decode(latent[i]), x_tar) # was set to x_in before\n",
    "#                                     agents_ae[j_agent].train()\n",
    "#                         test_l_sum_recons += recon_loss.cpu().item()\n",
    "                        \n",
    "#                         align_loss = align_loss + recon_loss\n",
    "\n",
    "#                         test_l_sum += align_loss.cpu().item()\n",
    "#                         test_batch_count += 1\n",
    "\n",
    "#                 if (epoch + 1) % print_frequency == 0:\n",
    "#                     print(\n",
    "#                         f'epoch {epoch + 1},' \n",
    "#                         f'align training loss {train_l_sum / train_batch_count:.4f},' \n",
    "#                         f'align sub-recons testing loss {test_l_sum_recons / test_batch_count:.4f},'\n",
    "#                         f'align sub-align testing loss {test_l_sum_align / test_batch_count:.4f},'\n",
    "#                         f'align testing loss {test_l_sum / test_batch_count:.4f}'\n",
    "#                     )\n",
    "#                     print('On training set alignment...')\n",
    "#                     test_alignment(agents_ae, align_trainloader, recons_criterion)\n",
    "#                     print('On testing set alignment...')\n",
    "#                     test_alignment(agents_ae, align_testloader, recons_criterion)\n",
    "                \n",
    "#                 pass\n",
    "\n",
    "#     except KeyboardInterrupt:\n",
    "#         pass\n",
    "\n",
    "#     for i in range(n_agents):\n",
    "#         i_agent = i + 1\n",
    "#         ae_ckpt_file: str = f\"ae_ckpt/group{n_group}_dim{ae_dim_str}_ReconsTest{n_recons_test}_AlignTest{n_align_test}_bs{ae_batch_size}_AlignLr{ae_align_lr}_epo{epoch+1}_{loss_type}/ae{i_agent}of{n_agents}.pt\"\n",
    "        \n",
    "#         os.makedirs(os.path.dirname(ae_ckpt_file), exist_ok=True)\n",
    "\n",
    "#         torch.save(agents_ae[i_agent].state_dict(), ae_ckpt_file)\n",
    "#         print(f'Autoencoder of agent {i_agent} saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method 4: balanced align + balanced reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_ae = {}\n",
    "for i in range(n_agents):\n",
    "    i_agent = i + 1\n",
    "    agents_ae[i_agent] = Autoencoder().to(device)\n",
    "    if load_ae:\n",
    "        ae_ckpt_file: str = f\"ae_ckpt/group{n_group}_dim{ae_dim_str}_ReconsTest{n_recons_test}_AlignTest{n_align_test}_bs{ae_batch_size}_AlignLr{ae_align_lr}_epo{n_epoch}_{loss_type}/ae{i_agent}of{n_agents}.pt\"\n",
    "        agents_ae[i_agent].load_state_dict(torch.load(ae_ckpt_file))\n",
    "        print(f'Autoencoder of agent {i_agent} loaded!')\n",
    "\n",
    "global_optim = torch.optim.Adam([p for model in agents_ae.values() for p in model.parameters()], lr = ae_align_lr, weight_decay = ae_weight_decay)\n",
    "\n",
    "recons_criterion = nn.__dict__[loss_type]()\n",
    "\n",
    "if train_ae:\n",
    "    try:\n",
    "        for epoch in range(n_epoch):\n",
    "            train_l_sum = 0.\n",
    "            train_batch_count = 0\n",
    "            test_l_sum = 0.\n",
    "            test_l_sum_recons = 0.\n",
    "            test_l_sum_align = 0.\n",
    "            test_batch_count = 0\n",
    "            for x in align_trainloader:\n",
    "                latent = []\n",
    "                for i in range(n_agents):\n",
    "                    i_agent = i + 1\n",
    "                    x_in = x[i].to(device)\n",
    "                    x_hat = agents_ae[i_agent].encode(x_in)\n",
    "                    latent.append(x_hat)\n",
    "\n",
    "                latent_shifted = []\n",
    "                shifted_num = torch.randint(1, n_agents, ()).item()\n",
    "                for i in range(n_agents):\n",
    "                    latent_shifted.append(latent[(i + shifted_num) % n_agents])\n",
    "\n",
    "                # method 2: align between each latent vector plus mutual reconstruction loss\n",
    "                align_loss = 0.0\n",
    "                for i in range(n_agents):\n",
    "                    align_loss += align_weight * align_balanced_l1loss(latent[i], latent_shifted[i])\n",
    "\n",
    "                recon_loss = 0.0\n",
    "                for i in range(n_agents):\n",
    "                    i_agent = i + 1\n",
    "                    x_in = x[i].to(device)\n",
    "                    recon_loss += recons_criterion(\n",
    "                        agents_ae[i_agent].decode(\n",
    "                            torch.cat([latent[i], latent_shifted[i]], dim = 0)\n",
    "                        ), \n",
    "                        torch.cat([x_in, x_in], dim = 0)\n",
    "                    )\n",
    "\n",
    "                align_loss = align_loss + recon_loss\n",
    "\n",
    "                global_optim.zero_grad()\n",
    "                align_loss.backward()\n",
    "                global_optim.step()\n",
    "\n",
    "                train_l_sum += align_loss.cpu().item()\n",
    "                train_batch_count += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x in align_testloader:\n",
    "                    latent = []\n",
    "                    for i in range(n_agents):\n",
    "                        i_agent = i + 1\n",
    "                        agents_ae[i_agent].eval()\n",
    "                        x_in = x[i].to(device)\n",
    "                        x_hat = agents_ae[i_agent].encode(x_in)\n",
    "                        latent.append(x_hat)\n",
    "                        agents_ae[i_agent].train()\n",
    "\n",
    "                    latent_shifted = []\n",
    "                    shifted_num = torch.randint(1, n_agents, ()).item()\n",
    "                    for i in range(n_agents):\n",
    "                        latent_shifted.append(latent[(i + shifted_num) % n_agents])\n",
    "\n",
    "                    align_loss = 0.0\n",
    "                    for i in range(n_agents):\n",
    "                        align_loss += align_weight * align_balanced_l1loss(latent[i], latent_shifted[i])\n",
    "                    test_l_sum_align += align_loss.cpu().item()\n",
    "\n",
    "                    recon_loss = 0.0\n",
    "                    for i in range(n_agents):\n",
    "                        i_agent = i + 1\n",
    "                        x_in = x[i].to(device)\n",
    "                        agents_ae[i_agent].eval()\n",
    "                        recon_loss += recons_criterion(\n",
    "                            agents_ae[i_agent].decode(\n",
    "                                torch.cat([latent[i], latent_shifted[i]], dim = 0)\n",
    "                            ), \n",
    "                            torch.cat([x_in, x_in], dim = 0)\n",
    "                        )\n",
    "                        agents_ae[i_agent].train()\n",
    "\n",
    "                    test_l_sum_recons += recon_loss.cpu().item()\n",
    "                    \n",
    "                    align_loss = align_loss + recon_loss\n",
    "\n",
    "                    test_l_sum += align_loss.cpu().item()\n",
    "                    test_batch_count += 1\n",
    "\n",
    "            if (epoch + 1) % print_frequency == 0:\n",
    "                print(\n",
    "                    f'epoch {epoch + 1},' \n",
    "                    f'align training loss {train_l_sum / train_batch_count:.4f},' \n",
    "                    f'align sub-recons testing loss {test_l_sum_recons / test_batch_count:.4f},'\n",
    "                    f'align sub-align testing loss {test_l_sum_align / test_batch_count:.4f},'\n",
    "                    f'align testing loss {test_l_sum / test_batch_count:.4f}'\n",
    "                )\n",
    "                print('On training set alignment...')\n",
    "                test_alignment(agents_ae, align_trainloader, recons_criterion)\n",
    "                print('On testing set alignment...')\n",
    "                test_alignment(agents_ae, align_testloader, recons_criterion)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    for i in range(n_agents):\n",
    "        i_agent = i + 1\n",
    "        ae_ckpt_file: str = f\"ae_ckpt/group{n_group}_dim{ae_dim_str}_ReconsTest{n_recons_test}_AlignTest{n_align_test}_bs{ae_batch_size}_AlignLr{ae_align_lr}_epo{epoch+1}_{loss_type}/ae{i_agent}of{n_agents}.pt\"\n",
    "        \n",
    "        os.makedirs(os.path.dirname(ae_ckpt_file), exist_ok=True)\n",
    "\n",
    "        torch.save(agents_ae[i_agent].state_dict(), ae_ckpt_file)\n",
    "        print(f'Autoencoder of agent {i_agent} saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. EXECUTION - Concept reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get unique concept of each agent\n",
    "\n",
    "get latent representation of each agent's unique concept\n",
    "\n",
    "get all recons concepts of each agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_conceptsets = {}\n",
    "unique_conceptloaders = {}\n",
    "\n",
    "for i in range(n_agents):\n",
    "    i_agent = i + 1\n",
    "    unique_indices = get_unique_indices(n_mazes, n_agents, i_agent, n_unique)\n",
    "    unique_conceptsets[i_agent] = ReconsDataset(agents[i_agent].concept_embedding_layer.weight.data[unique_indices])\n",
    "    unique_conceptloaders[i_agent] = DataLoader(unique_conceptsets[i_agent], batch_size = ae_batch_size, shuffle = False)\n",
    "\n",
    "latent_unique = {}\n",
    "\n",
    "for i in range(n_agents):\n",
    "    i_agent = i + 1\n",
    "    latent_i = None\n",
    "    for x in unique_conceptloaders[i_agent]:\n",
    "        x = x.to(device)\n",
    "        latent_hat = agents_ae[i_agent].encode(x)\n",
    "        if latent_i is None:\n",
    "            latent_i = latent_hat\n",
    "        else:\n",
    "            latent_i = torch.cat([latent_i, latent_hat], dim = 0)\n",
    "    \n",
    "    latent_unique[i_agent] = latent_i\n",
    "\n",
    "common_indices = get_common_indices(n_mazes, n_agents, n_unique)\n",
    "latent_common = {}\n",
    "recons_concepts = {}\n",
    "\n",
    "for i in range(n_agents):\n",
    "    latent_common_i = None\n",
    "    recons_concept_i = None\n",
    "    i_agent = i + 1\n",
    "    common_conceptset = ReconsDataset(agents[i_agent].concept_embedding_layer.weight.data[common_indices])\n",
    "    common_conceptloaders = DataLoader(common_conceptset, batch_size = ae_batch_size, shuffle = False)\n",
    "    for x in common_conceptloaders:\n",
    "        x = x.to(device)\n",
    "        latent_hat = agents_ae[i_agent].encode(x)\n",
    "        x_hat = agents_ae[i_agent].decode(latent_hat)\n",
    "        \n",
    "        if latent_common_i is None:\n",
    "            latent_common_i = latent_hat\n",
    "        else:\n",
    "            latent_common_i = torch.cat([latent_common_i, latent_hat], dim = 0)\n",
    "\n",
    "        if recons_concept_i is None:\n",
    "            recons_concept_i = x_hat\n",
    "        else:\n",
    "            recons_concept_i = torch.cat([recons_concept_i, x_hat], dim = 0)\n",
    "    \n",
    "    # cat is wrong, should be concated by indices\n",
    "    for j in range(n_agents):\n",
    "        j_agent = j + 1\n",
    "        latent_j = agents_ae[i_agent].decode(latent_unique[j_agent])\n",
    "        recons_concept_i = torch.cat([recons_concept_i, latent_j], dim = 0)\n",
    "    \n",
    "    latent_common[i_agent] = latent_common_i.detach().cpu()\n",
    "    recons_concepts[i_agent] = recons_concept_i.detach().cpu()\n",
    "\n",
    "epoch = n_epoch - 1 if 'epoch' not in locals() or epoch is None else epoch\n",
    "torch.save(recons_concepts, f\"recons_concepts/concepts_group{n_group}_dim{ae_dim_str}_ReconsTest{n_recons_test}_AlignTest{n_align_test}_bs{ae_batch_size}_AlignLr{ae_align_lr}_epo{epoch+1}_{loss_type}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. TEST - Cross-agent concept reconstruction with aligndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_agent_maze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
